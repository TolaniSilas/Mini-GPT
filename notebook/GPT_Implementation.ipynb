{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9J2Jx2oEmIq"
      },
      "source": [
        "# **Mini GPT Implementation**\n",
        "\n",
        "This notebook implements and experiments the Generative Pretrained Transformer (GPT) series (e.g., GPT-2, GPT-3). The model architecture is the foundational building block underlying the GPT family, and its design is inspired by a major breakthrough in deep learning, natural language processing and machine translation: the **self-attention mechanism** introduced in **“Attention Is All You Need”** by Vaswani et al.\n",
        "\n",
        "The models were trained on Outliers Book by Malcolm Gladwell only for experimental purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt8rv0AsQosx",
        "outputId": "a646c286-e016-4fa6-bb74-ecb535c95a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/330.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade pypdf tiktoken==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF8QaNcHGCKy"
      },
      "outputs": [],
      "source": [
        "# install all the required libraries, modules and packages for this project.\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "from pypdf import PdfReader\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKcsWzNJdcOp",
        "outputId": "0026dd96-5237-4b5b-e80c-9ec6fc9b0910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package versions:\n",
            "\n",
            "pypdf==6.7.0\n",
            "matplotlib==3.10.0\n",
            "tiktoken==0.6.0\n",
            "torch==2.9.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# list of packages.\n",
        "packages = ['pypdf', 'matplotlib', 'tiktoken', 'torch']\n",
        "\n",
        "print(\"package versions:\\n\")\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['pip', 'show', package],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        # extract version from output.\n",
        "        for line in result.stdout.split('\\n'):\n",
        "            if line.startswith('Version:'):\n",
        "                version = line.split(':')[1].strip()\n",
        "                print(f\"{package}=={version}\")\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"{package}: error - {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQYJ_IxbFO5p"
      },
      "source": [
        "### **Load the book data that will be used to train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I--KM-ONQnNG",
        "outputId": "90e5759b-85de-43c6-b162-cae3669afc2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R7S1m0bN7XL"
      },
      "outputs": [],
      "source": [
        "# path to pdf file directory in google drive.\n",
        "pdf_path = \"/content/drive/MyDrive/Datasets/Book/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6ij94vxdDlr"
      },
      "outputs": [],
      "source": [
        "def pdf_to_text(pdf_path, output_txt_path=None, save_to_file=True):\n",
        "    \"\"\"reads pdf and converts to text, optionally saving to .txt file.\"\"\"\n",
        "\n",
        "    # check if pdf exists.\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"error: '{pdf_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # open pdf in binary mode.\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "\n",
        "            # create pdf reader.\n",
        "            reader = PdfReader(file)\n",
        "            num_pages = len(reader.pages)\n",
        "            print(f\"processing {num_pages} pages...\")\n",
        "\n",
        "            # extract all text.\n",
        "            full_text = []\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "\n",
        "                if text:\n",
        "                    full_text.append(text)\n",
        "                else:\n",
        "                    print(f\"warning: page {page_num + 1} had no extractable text.\")\n",
        "\n",
        "            # combine all pages.\n",
        "            combined_text = \"\\n\\n\".join(full_text)\n",
        "\n",
        "            # save to file if requested.\n",
        "            if save_to_file:\n",
        "                if output_txt_path is None:\n",
        "                    output_txt_path = pdf_path.replace('.pdf', '.txt')\n",
        "\n",
        "                with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "                    txt_file.write(combined_text)\n",
        "\n",
        "                print(f\"saved to: {output_txt_path}\")\n",
        "\n",
        "            return combined_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error reading pdf: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4BJIuhaMw0b"
      },
      "outputs": [],
      "source": [
        "def batch_pdf_to_text(folder_path, output_folder=None):\n",
        "    \"\"\"converts all pdfs in a folder to text files.\"\"\"\n",
        "\n",
        "    # check if folder exists.\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"error: folder '{folder_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    # create output folder if needed.\n",
        "    if output_folder and not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # get all pdf files.\n",
        "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"no pdf files found in folder.\")\n",
        "        return\n",
        "\n",
        "    print(f\"found {len(pdf_files)} pdf files. converting...\")\n",
        "\n",
        "    combined_texts = []\n",
        "\n",
        "    # process each pdf.\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(folder_path, pdf_file)\n",
        "\n",
        "        if output_folder:\n",
        "            output_path = os.path.join(output_folder, pdf_file.replace('.pdf', '.txt'))\n",
        "        else:\n",
        "            output_path = None\n",
        "\n",
        "        print(f\"\\nprocessing: {pdf_file}\")\n",
        "\n",
        "        combined_texts.append(pdf_to_text(pdf_path, output_path))\n",
        "\n",
        "    print(\"\\nbatch conversion complete!\")\n",
        "\n",
        "    return combined_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5uQYUstMoru",
        "outputId": "873dd0ea-6120-4284-b34b-c12ca1669239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 1 pdf files. converting...\n",
            "\n",
            "processing: Outliers_Malcolm_Gladwell.pdf\n",
            "processing 249 pages...\n",
            "warning: page 1 had no extractable text.\n",
            "saved to: /content/drive/MyDrive/Datasets/Book/Outliers_Malcolm_Gladwell.txt\n",
            "\n",
            "batch conversion complete!\n"
          ]
        }
      ],
      "source": [
        "raw_text_contents = batch_pdf_to_text(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMTVLtIP5fZQ"
      },
      "outputs": [],
      "source": [
        "raw_text_content = raw_text_contents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3dI5aAvZtdU",
        "outputId": "81aedd96-a145-438a-abdb-f23189000516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 460987 \n",
            "\n",
            "OUTLIERS\n",
            "The Story of Success\n",
            "MALCOLM GLADWELL\n",
            "BACK BAY BOOKS\n",
            "LITTLE, BROWN AND COMPANY\n",
            "NEW YORK   \n"
          ]
        }
      ],
      "source": [
        "print(\"Total number of character:\", len(raw_text_content), \"\\n\")\n",
        "print(raw_text_content[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkozSO2Ja0JF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_N_c68wfJcC"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(raw_text, max_preview=30):\n",
        "    \"\"\"splits text on punctuation and whitespace, removes empty strings.\"\"\"\n",
        "\n",
        "    # split on common punctuation and whitespace.\n",
        "    pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    tokens = re.split(pattern, raw_text)\n",
        "\n",
        "    # remove whitespace and filter empty strings.\n",
        "    preprocessed = [item.strip() for item in tokens if item.strip()]\n",
        "\n",
        "    # preview first n tokens.\n",
        "    if max_preview:\n",
        "        print(f\"first {max_preview} tokens:\")\n",
        "        print(preprocessed[:max_preview])\n",
        "        print(f\"\\ntotal word level tokens: {len(preprocessed)}\\n\")\n",
        "\n",
        "    return preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY6gLVk2kyWs",
        "outputId": "ba0efaf0-dc48-43df-b5be-99a71572af90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 30 tokens:\n",
            "['OUTLIERS', 'The', 'Story', 'of', 'Success', 'MALCOLM', 'GLADWELL', 'BACK', 'BAY', 'BOOKS', 'LITTLE', ',', 'BROWN', 'AND', 'COMPANY', 'NEW', 'YORK', '•', 'BOSTON', '•', 'LONDON', 'Begin', 'Reading', 'Table', 'of', 'Contents', 'Reading', 'Group', 'Guide', 'Copyright']\n",
            "\n",
            "total word level tokens: 92758\n",
            "\n",
            "11324\n"
          ]
        }
      ],
      "source": [
        "preprocessed = preprocess_text(raw_text_content, max_preview=30)\n",
        "\n",
        "# get the vocabulary size of the dataset.\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raXXtMvDma8g"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z49XMHL-vgai"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZQAQ4xennON"
      },
      "outputs": [],
      "source": [
        "class WordTokenizer:\n",
        "    \"\"\"tokenizes text into ids and decodes ids back to text. it focuses on word-level tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"initializes tokenizer with vocabulary mappings.\"\"\"\n",
        "\n",
        "        self.tok_to_int = vocab\n",
        "        self.int_to_tok = {integer: token for token, integer in vocab.items()}\n",
        "        self.pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"converts text to list of token ids.\"\"\"\n",
        "\n",
        "        # split on punctuation and whitespace.\n",
        "        preprocessed = re.split(self.pattern, text)\n",
        "\n",
        "        # remove empty strings and whitespace.\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "        # replace unknown tokens with <|unk|>.\n",
        "        preprocessed = [token if token in self.tok_to_int else \"<|unk|>\" for token in preprocessed]\n",
        "\n",
        "        # convert tokens to ids.\n",
        "        ids = [self.tok_to_int[tok] for tok in preprocessed]\n",
        "\n",
        "        # return ids\n",
        "        return ids\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"converts list of token ids back to text.\"\"\"\n",
        "\n",
        "        # map ids to tokens.\n",
        "        tokens = [self.int_to_tok[id] for id in ids]\n",
        "\n",
        "        # join tokens with spaces.\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(self.pattern, r'\\1', text)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(r'\\s+([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-])', r'\\1', text)\n",
        "\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkcOsA0G9tGl",
        "outputId": "56d74a3e-4da2-46ef-e077-7d02b2899dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: Chris Langan's mother was from San Francisco and was estranged from her family. <|endoftext|> do you know about smartphone cryptocurrency?\n",
            "\n",
            "encoded text: [915, 1905, 11325, 11325, 7500, 10581, 5993, 2676, 1342, 3561, 10581, 5524, 5993, 6370, 5700, 29, 11325, 11325, 11325, 11325, 11325, 5192, 10835, 6940, 3291, 11325, 11325, 430]\n",
            "\n",
            "decoded text: Chris Langan <|unk|> <|unk|> mother was from San Francisco and was estranged from her family. <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> do you know about <|unk|> <|unk|>?\n"
          ]
        }
      ],
      "source": [
        "# initialize the wordtokenizer class.\n",
        "tokenizer = WordTokenizer(vocab)\n",
        "\n",
        "# sample text from (and outside) outliers book.\n",
        "text1 = \"Chris Langan's mother was from San Francisco and was estranged from her family.\"\n",
        "\n",
        "# sample text with unknown words for testing.\n",
        "text2 = \"do you know about smartphone cryptocurrency?\"\n",
        "\n",
        "\n",
        "# combine texts with special separator token.\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(f\"original text: {text}\")\n",
        "\n",
        "# convert text to token ids.\n",
        "encoded_text = tokenizer.encode(text)\n",
        "print(f\"\\nencoded text: {encoded_text}\")\n",
        "\n",
        "# convert token ids back to text.\n",
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(f\"\\ndecoded text: {decoded_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFdoj2Q1-PXH"
      },
      "source": [
        "### **Byte Pair Encoding (BPE) Tokenizer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T_pVWgO-Ae3"
      },
      "source": [
        "The GPT Series uses a more sophisticated encoding preprocessing technique to convert human language (texts or words) into sub-word units for its training processes. It uses the Byte Pair Encoding (BPE) tokenizer. BPE tokenizer is fast, performant and compressed compared to other tokenization methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mFnqUFJ-EoW"
      },
      "source": [
        "Its implementation is quite complex and time-consuming, so I used the tiktoken open-source library to bypass this process (the logic is almost akin to the WordTokenizer implemented above). Tiktoken is an invertible and lossless tokenization technique. it can work on arbitrary text - even text that is not present in the tokenizer's training data. it compresses the text, whereby making the token sequence shorter than the bytes corresponding to the original text.\n",
        "\n",
        "It also attempts to let the model see common subwords. For instance, \"ing\" is a common subword in English, so BPE encodings will often split \"encoding\" into tokens like \"encod\" and \"ing\" (instead of e.g. \"enc\" and \"oding\"). Because the model will then see the \"ing\" token again and again in different contexts, it helps in better contextual understanding of grammar and in generalization of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jc1GBmL-bYK",
        "outputId": "d30c2777-4ed6-4052-daf5-79c9e214af4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bpe tokenizer's encoded text: [15645, 16332, 272, 338, 2802, 373, 422, 2986, 6033, 290, 373, 44585, 422, 607, 1641, 13, 220, 50256, 466, 345, 760, 546, 11745, 20210, 30]\n",
            "\n",
            "bpe tokenizer's decoded text: Chris Langan's mother was from San Francisco and was estranged from her family. <|endoftext|> do you know about smartphone cryptocurrency?\n",
            "\n",
            "the vocabulary size of gpt-2 tokenizer is 50257\n"
          ]
        }
      ],
      "source": [
        "# instantiate bpe tokenizer from tiktoken.\n",
        "tokenizer_encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# convert text to token ids.\n",
        "encoded_text = tokenizer_encoder.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(f\"\\nbpe tokenizer's encoded text: {encoded_text}\")\n",
        "\n",
        "# convert token ids back to text.\n",
        "decoded_text = tokenizer_encoder.decode(encoded_text)\n",
        "print(f\"\\nbpe tokenizer's decoded text: {decoded_text}\")\n",
        "\n",
        "# display the gpt-2 bpe tovocabulary size.\n",
        "print(f\"\\nthe vocabulary size of gpt-2 tokenizer is {tokenizer_encoder.n_vocab}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c97zmGj-bQ6"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "    \"\"\"tokenizes text using byte pair encoding (bpe). uses tiktoken's gpt-2 tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"gpt2\"):\n",
        "        \"\"\"initializes bpe tokenizer with specified model.\"\"\"\n",
        "\n",
        "        # load tiktoken encoder for the specified model.\n",
        "        self.tokenizer = tiktoken.get_encoding(model_name)\n",
        "\n",
        "        # obtain and store the tokenizer's vocab size.\n",
        "        self.vocab_size = self.tokenizer.n_vocab\n",
        "\n",
        "    def encode(self, text, allowed_special=None):\n",
        "        \"\"\"converts text to list of token ids.\"\"\"\n",
        "\n",
        "        # set default allowed special tokens.\n",
        "        if allowed_special is None:\n",
        "            allowed_special = {\"<|endoftext|>\"}\n",
        "\n",
        "        # convert text to token ids.\n",
        "        ids = self.tokenizer.encode(text, allowed_special=allowed_special)\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"converts list of token ids back to text.\"\"\"\n",
        "\n",
        "        # convert token ids back to text.\n",
        "        text = self.tokenizer.decode(ids)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"returns the vocabulary size of the tokenizer.\"\"\"\n",
        "\n",
        "        return self.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOkQSUCE8SR9",
        "outputId": "84aafab8-eaf8-4b10-9b08-0d92fad00e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total characters: 460987\n",
            "The total tokens: 115003\n"
          ]
        }
      ],
      "source": [
        "# initialize tokenizer.\n",
        "tokenizer = BPETokenizer()\n",
        "\n",
        "# get total character count.\n",
        "total_characters = len(raw_text_content)\n",
        "\n",
        "# get total token count after encoding.\n",
        "total_tokens = len(tokenizer.encode(raw_text_content))\n",
        "\n",
        "print(\"The total characters:\", total_characters)\n",
        "print(\"The total tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPdy1oIUCT37"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkP3CFLfZaLV"
      },
      "source": [
        "We reduce the context length (context_length) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU3F8XeRCTvn"
      },
      "outputs": [],
      "source": [
        "gpt_config_124M = {\n",
        "    \"vocab_size\": 50257,  # the model's vocabulary size.\n",
        "    \"embed_dim\": 768,  # the embedding function.\n",
        "    \"num_heads\": 12, # the number of attention heads.\n",
        "    \"num_layers\": 12, # the number of transformer layers.\n",
        "    \"context_length\": 256, # the model's context length. this is the shortened context length (original context length is 1024).\n",
        "    \"drop_rate\": 0.2, # the probability of drop out rate.\n",
        "    \"qkv_bias\": False, # the query-key-value bias.\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zisDZoBL6R8W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei5zrw9124mm"
      },
      "outputs": [],
      "source": [
        "class GPTTextDataset(Dataset):\n",
        "    \"\"\"pytorch custom dataset for tokenized text data.\"\"\"\n",
        "\n",
        "    def __init__(self, txt, tokenizer, context_length=256, stride=128):\n",
        "        \"\"\"initializes dataset with text files and tokenizer.\"\"\"\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context_length = context_length\n",
        "        self.stride = stride\n",
        "\n",
        "        # tokenize all text and create chunks.\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # tokenize the entire text (document).\n",
        "        token_ids = self.tokenizer.encode(txt)\n",
        "\n",
        "        # create a sliding window to chunk the text (document) into overlapping sequences of context length.\n",
        "        for i in range(0, len(token_ids) - self.context_length, self.stride):\n",
        "\n",
        "            # input is current chunk.\n",
        "            input_chunk = token_ids[i:i + self.context_length]\n",
        "\n",
        "            # target is shifted by one position.\n",
        "            target_chunk = token_ids[i + 1:i + self.context_length + 1]\n",
        "\n",
        "            # ensure both chunks are same length.\n",
        "            if len(input_chunk) == self.context_length and len(target_chunk) == self.context_length:\n",
        "                self.input_ids.append(input_chunk)\n",
        "                self.target_ids.append(target_chunk)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"returns the number of samples in dataset.\"\"\"\n",
        "\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"returns input and target tensors for given index.\"\"\"\n",
        "\n",
        "        # convert to tensors with long data type.\n",
        "        input_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
        "        target_tensor = torch.tensor(self.target_ids[idx], dtype=torch.long)\n",
        "\n",
        "        return input_tensor, target_tensor\n",
        "\n",
        "\n",
        "\n",
        "def create_dataloader(dataset, batch_size=8, shuffle=True, drop_last=True, num_workers=0):\n",
        "    \"\"\"creates pytorch dataloader from the custom dataset.\"\"\"\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=False,\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddctlh1o24iE"
      },
      "outputs": [],
      "source": [
        "text_data = GPTTextDataset(raw_text_content, tokenizer, gpt_config_124M[\"context_length\"], stride=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfNUO74wwu-Q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch1TEqWW24dV"
      },
      "outputs": [],
      "source": [
        "# set train/validation split ratio.\n",
        "train_ratio = 0.90\n",
        "dataset_len = len(text_data)\n",
        "\n",
        "# calculate split sizes.\n",
        "train_len = int(train_ratio * dataset_len)\n",
        "val_len = dataset_len - train_len\n",
        "\n",
        "# set seed for reproducible split.\n",
        "torch.manual_seed(66)\n",
        "\n",
        "# split dataset into train and validation sets.\n",
        "train_data, val_data = random_split(\n",
        "    text_data,\n",
        "    [train_len, val_len]\n",
        ")\n",
        "\n",
        "# set seed for reproducible data loading.\n",
        "torch.manual_seed(66)\n",
        "\n",
        "# create training data loader.\n",
        "train_loader = create_dataloader(\n",
        "    train_data,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# create validation data loader.\n",
        "val_loader = create_dataloader(\n",
        "    val_data,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQY9eZB8CTsB"
      },
      "outputs": [],
      "source": [
        "# sanity check for sufficient training tokens.\n",
        "if total_tokens * train_ratio < gpt_config_124M[\"context_length\"]:\n",
        "    print(\"not enough tokens for the training loader. \"\n",
        "          \"try to lower the `gpt_config_124m['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "# sanity check for sufficient validation tokens.\n",
        "if total_tokens * (1 - train_ratio) < gpt_config_124M[\"context_length\"]:\n",
        "    print(\"not enough tokens for the validation loader. \"\n",
        "          \"try to lower the `gpt_config_124m['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJIGb1lg6AOX",
        "outputId": "586a5dfe-71a2-4403-d558-09f4a38c5d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training tokens: 204800\n",
            "validation tokens: 22528\n",
            "all tokens: 227328\n"
          ]
        }
      ],
      "source": [
        "# count total tokens in training set.\n",
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "# count total tokens in validation set.\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "# display token counts.\n",
        "print(\"training tokens:\", train_tokens)\n",
        "print(\"validation tokens:\", val_tokens)\n",
        "print(\"all tokens:\", train_tokens + val_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-v18dLJsuJ6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WSVLL2ysuFC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahn-xf-Pst_Z"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SHL2oXtbjKj"
      },
      "source": [
        "### Guassian Error Linear Unit (GELU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cJA8yu66t0k"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    \"\"\"gaussian error linear unit (gelu) activation function. mathematical implementation of gelu.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"initializes gelu activation.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies gelu activation to input tensor.\"\"\"\n",
        "\n",
        "        # compute gelu activation using tanh approximation.\n",
        "        return 0.5 * x * (\n",
        "            1 + torch.tanh(\n",
        "                torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "                (x + 0.044715 * torch.pow(x, 3))\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCCOEr9tbd7W"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwjbhK9g6tCw"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"layer normalization for stabilizing neural network training.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim):\n",
        "        \"\"\"initializes layer normalization with learnable parameters.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # small constant to prevent division by zero.\n",
        "        self.eps = 1e-5\n",
        "\n",
        "        # learnable scale parameter.\n",
        "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
        "\n",
        "        # learnable shift parameter.\n",
        "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies layer normalization to input tensor.\"\"\"\n",
        "\n",
        "        # calculate mean across last dimension.\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "\n",
        "        # calculate variance across last dimension.\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "\n",
        "        # normalize input.\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "        # apply scale and shift.\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdeE-rATse1O"
      },
      "source": [
        "### Feed Forward Sub-Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfm29lIoCTj_"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"position-wise feed-forward network in transformer. the second sub-layer in each transformer block.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"initializes feed-forward network with two linear layers.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # save model configuration for future extraction.\n",
        "        self.config = config\n",
        "\n",
        "        # two-layer mlp with gelu activation. it expands to 4x embed_dim then projects back.\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.config[\"embed_dim\"], 4 * self.config[\"embed_dim\"]),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * self.config[\"embed_dim\"], self.config[\"embed_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies feed-forward transformation to input.\"\"\"\n",
        "\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jii7xVrisdC1"
      },
      "source": [
        "### Self Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6lLYN4gCTfX"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"self-attention mechanism for processing sequences.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        \"\"\"initializes self-attention with query, key, and value projections.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # query projection layer.\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # key projection layer.\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # value projection layer.\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies self-attention to input sequence.\"\"\"\n",
        "\n",
        "        # compute queries, keys, and values.\n",
        "        queries = self.W_query(x)\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # get key dimension for scaling.\n",
        "        d_k = keys.shape[-1]\n",
        "        scaling_factor = d_k**0.5\n",
        "\n",
        "        # compute attention scores.\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "\n",
        "        # apply scaled softmax to get attention weights.\n",
        "        attn_weights = torch.softmax(attn_scores / scaling_factor, dim=-1)\n",
        "\n",
        "        # compute weighted sum of values.\n",
        "        context_vector = attn_weights @ values\n",
        "\n",
        "        return context_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62tGdt4IsW1s"
      },
      "source": [
        "### Causal Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCE0-yU1mpOW"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "    \"\"\"causal self-attention with masking to prevent attending to future tokens (preventing the model from peeking or cheating by having context on future tokens).\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "        \"\"\"initializes causal attention with query, key, value projections and mask.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # output dimension.\n",
        "        self.d_out = d_out\n",
        "\n",
        "        # query projection layer.\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # key projection layer.\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # value projection layer.\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # dropout layer.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # causal mask to prevent attending to future positions.\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies causal self-attention to input sequence.\"\"\"\n",
        "\n",
        "        # get batch size, sequence length, and input dimension.\n",
        "        batch, num_tokens, d_in = x.shape\n",
        "\n",
        "        # compute queries, keys, and values.\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # get key dimension for scaling.\n",
        "        d_k = keys.shape[-1]\n",
        "        scaling_factor = d_k**0.5\n",
        "\n",
        "        # compute attention scores.\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "\n",
        "        # apply causal mask to prevent attending to future tokens.\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "        )\n",
        "\n",
        "        # apply scaled softmax to get attention weights.\n",
        "        attn_weights = torch.softmax(attn_scores / scaling_factor, dim=-1)\n",
        "\n",
        "        # apply dropout to attention weights.\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # compute weighted sum of values.\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRSmbYSkmriW"
      },
      "source": [
        "### Multi-Head Attention (MHA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sovmriHqoE_F"
      },
      "outputs": [],
      "source": [
        "class MultiHeadWrapper(nn.Module):\n",
        "    \"\"\"multi-head wrapper mechanism with multiple parallel attention heads.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        \"\"\"initializes multi-head attention with multiple causal attention heads.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # list of attention heads running in parallel.\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "             for i in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies all attention heads and concatenates their outputs.\"\"\"\n",
        "\n",
        "        # run each head and concatenate results along feature dimension.\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmc4GkxDpyTN"
      },
      "source": [
        "Even though the MultiHeadAttention class looks more complicated or tangled than the MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors, it is more efficient and performant.\n",
        "\n",
        "The reason is that we only need one matrix multiplication to compute the keys, for instance, keys = self.W_key(x) (the same is true for the queries and values). Unlike, the MultiHeadAttentionWrapper, where we needed to repeat this matrix multiplication, which is computationally (and time-consuming) one of the most expensive steps, for each attention head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMUgU7biYmuv"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multi-head attention mechanism with multiple parallel attention heads.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        \"\"\"initializes multi-head attention with projections and causal mask.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # ensure output dimension is divisible by number of heads.\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        # store dimensions.\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        # query projection layer.\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # key projection layer.\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # value projection layer.\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # output projection to combine heads.\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "        # dropout layer.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # causal mask to prevent attending to future positions (in order to avoid cheating).\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies multi-head attention to input sequence.\"\"\"\n",
        "\n",
        "        # get batch size, sequence length, and input dimension.\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        # compute queries, keys, and values.\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # split into multiple heads by reshaping.\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # rearrange to (batch, num_heads, seq_len, head_dim).\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # compute attention scores.\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        # apply causal mask to prevent attending to future tokens.\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        # get key dimension for scaling.\n",
        "        d_k = keys.shape[-1]\n",
        "        scaling_factor = d_k**0.5\n",
        "\n",
        "        # apply scaled softmax to get attention weights.\n",
        "        attn_weights = torch.softmax(attn_scores / scaling_factor, dim=-1)\n",
        "\n",
        "        # apply dropout to attention weights.\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # compute weighted sum of values.\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        # rearrange back to (batch, seq_len, num_heads, head_dim).\n",
        "        context_vec = context_vec.transpose(1, 2)\n",
        "\n",
        "        # combine all heads by concatenating.\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "        # apply output projection.\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmd3rbFrsTlH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmL4-bI-sVDF"
      },
      "source": [
        "### The Transformer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98GocTZE6vfx"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    \"\"\"transformer decoder block with multi-head attention and feed-forward layers.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"initializes transformer block with attention and feed-forward components.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # multi-head attention layer.\n",
        "        self.multi_att = MultiHeadAttention(\n",
        "            d_in=config[\"embed_dim\"],\n",
        "            d_out=config[\"embed_dim\"],\n",
        "            context_length=config[\"context_length\"],\n",
        "            num_heads=config[\"num_heads\"],\n",
        "            dropout=config[\"drop_rate\"],\n",
        "            qkv_bias=config[\"qkv_bias\"]\n",
        "        )\n",
        "\n",
        "        # layer normalization layers.\n",
        "        self.norm1 = nn.LayerNorm(config[\"embed_dim\"])\n",
        "        self.norm2 = nn.LayerNorm(config[\"embed_dim\"])\n",
        "\n",
        "        # dropout for residual connections.\n",
        "        self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "        # feed-forward network.\n",
        "        self.feed_for = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"applies transformer block with residual connections.\"\"\"\n",
        "\n",
        "        ## -------  shortcut connection for multi-head attention sub-layer.  -------\n",
        "\n",
        "        # store the original input.\n",
        "        shortcut = x\n",
        "\n",
        "        # apply layer normalization before the multi-head attention sub-layer.\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # apply the output of the layer normalization to the multi-head attention sub-layer.\n",
        "        x = self.multi_att(x)\n",
        "\n",
        "        # apply drop out.\n",
        "        x = self.drop_shortcut(x)\n",
        "\n",
        "        # add the original input back.\n",
        "        x = x + shortcut\n",
        "\n",
        "        ## ------  shortcut connection for feed forward sub-layer.  ------\n",
        "\n",
        "        # store the original input.\n",
        "        shortcut = x\n",
        "\n",
        "        # apply layer normalization before the feed forward sub-layer.\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # apply the output of the layer normalization to the feed forward sub-layer.\n",
        "        x = self.feed_for(x)\n",
        "\n",
        "        # apply drop out.\n",
        "        x = self.drop_shortcut(x)\n",
        "\n",
        "        # add the original input back.\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqgvk6X0tCif"
      },
      "source": [
        "### GPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5B6IxcW6vbP"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    \"\"\"gpt model with transformer decoder blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"initializes gpt model with embeddings and transformer layers.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # token embedding layer.\n",
        "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"embed_dim\"])\n",
        "\n",
        "        # positional embedding layer.\n",
        "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"embed_dim\"])\n",
        "\n",
        "        # dropout for embeddings.\n",
        "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "        # stack of transformer decoder blocks.\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerDecoderBlock(config) for i in range(config[\"num_layers\"])]\n",
        "        )\n",
        "\n",
        "        # final layer normalization.\n",
        "        self.final_norm = nn.LayerNorm(config[\"embed_dim\"])\n",
        "\n",
        "        # output projection to vocabulary.\n",
        "        self.out_head = nn.Linear(config[\"embed_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        \"\"\"forward pass through the gpt model.\"\"\"\n",
        "\n",
        "        # get batch size and sequence length.\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "\n",
        "        # get token embeddings.\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "\n",
        "        # get positional embeddings.\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "\n",
        "        # combine token and positional embeddings.\n",
        "        x = tok_embeds + pos_embeds\n",
        "\n",
        "        # apply dropout to embeddings.\n",
        "        x = self.drop_emb(x)\n",
        "\n",
        "        # pass through transformer blocks.\n",
        "        x = self.trf_blocks(x)\n",
        "\n",
        "        # apply final normalization.\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # project to vocabulary size.\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSJLJwLuscNa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AbwIxTYljLD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfIsq6c8scKk"
      },
      "outputs": [],
      "source": [
        "def calculate_loss_batch(input_batch, target_batch, model, device):\n",
        "    \"\"\"calculates loss for a single batch.\"\"\"\n",
        "\n",
        "    # move batch to device.\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    # get model predictions.\n",
        "    logits = model(input_batch)\n",
        "\n",
        "    # calculate cross-entropy loss.\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calculate_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    \"\"\"calculates average loss across multiple batches from data loader.\"\"\"\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # handle empty data loader.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    # determine number of batches to process.\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # limit to available batches.\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    # iterate through batches and accumulate loss.\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "\n",
        "        if i < num_batches:\n",
        "            # calculate loss for current batch.\n",
        "            loss = calculate_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "            if i == 0:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # return average loss.\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcp6RZarscIN",
        "outputId": "253a8f66-5704-4faa-9d82-fe8511e6bbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cuda device.\n",
            "training loss: 0.109576416015625\n",
            "validation loss: 0.9976380088112571\n"
          ]
        }
      ],
      "source": [
        "# initialize gpt model.\n",
        "model = GPTModel(gpt_config_124M)\n",
        "\n",
        "# set device to gpu if available, otherwise cpu.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using {device} device.\")\n",
        "\n",
        "# move model to device.\n",
        "model.to(device)\n",
        "\n",
        "# set seed for reproducibility.\n",
        "torch.manual_seed(66)\n",
        "\n",
        "# calculate initial losses without gradient tracking (no model training is happening yet).\n",
        "with torch.no_grad():\n",
        "\n",
        "    # calculate training loss.\n",
        "    train_loss = calculate_loss_loader(train_loader, model, device)\n",
        "\n",
        "    # calculate validation loss.\n",
        "    val_loss = calculate_loss_loader(val_loader, model, device)\n",
        "\n",
        "# display initial losses.\n",
        "print(\"training loss:\", train_loss)\n",
        "print(\"validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0muWfuZXsb97"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    \"\"\"evaluates model on training and validation sets.\"\"\"\n",
        "\n",
        "    # set model to evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # calculate losses without gradient tracking.\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # calculate training loss.\n",
        "        train_loss = calculate_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "\n",
        "        # calculate validation loss.\n",
        "        val_loss = calculate_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "\n",
        "    # set model back to training mode.\n",
        "    model.train()\n",
        "\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9rDP3RP_J-C"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    \"\"\"generates text by predicting tokens one at a time.\"\"\"\n",
        "\n",
        "    # set model to evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # generate tokens one by one.\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # crop context if it exceeds maximum context size.\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # get model predictions without gradient tracking.\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # focus only on last time step.\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # get token with highest probability (greedy decoding).\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # append sampled token to running sequence.\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZmhjDsex4Ei",
        "outputId": "66b09e84-1f5d-4a18-f25c-6d7585cd9db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text:\n",
            " who's an outlier? Cooking Trevor Code NeptuneNintendo Rodney McN chefs hack temporary\n"
          ]
        }
      ],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    \"\"\"converts text to token ids with batch dimension.\"\"\"\n",
        "\n",
        "    # encode text to token ids.\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    # convert to tensor and add batch dimension.\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    \"\"\"converts token ids back to text.\"\"\"\n",
        "\n",
        "    # remove batch dimension.\n",
        "    flat = token_ids.squeeze(0)\n",
        "\n",
        "    # decode token ids to text.\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "# set starting prompt for text generation.\n",
        "starting_prompt = \"who's an outlier?\"\n",
        "\n",
        "# convert text to token ids and move to device.\n",
        "input_ids = text_to_token_ids(starting_prompt, tokenizer).to(device)\n",
        "\n",
        "# generate text from prompt.\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=input_ids,\n",
        "    max_new_tokens=10,\n",
        "    context_size=gpt_config_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# display generated text.\n",
        "print(\"output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TayPpePxUdH"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
        "\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))\n",
        "\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DtZlFbCxUU5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN3O_YhRabDE"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, starting_prompt, tokenizer):\n",
        "\n",
        "    # initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # set model to training mode.\n",
        "        model.train()\n",
        "\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "\n",
        "            # reset loss gradients from previous batch iteration.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = calculate_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "            # calculate loss gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # update model weights using loss gradients.\n",
        "            optimizer.step()\n",
        "\n",
        "            # returns the total number of elements (or tokens) in the input_batch.\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # optional evaluation step.\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # print a sample text after each epoch.\n",
        "        generate_and_print_sample(model, tokenizer, device, starting_prompt)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6nWXviwAE4I",
        "outputId": "beb48996-7e07-40ad-ecaa-23ca999fcc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Step 000000): Train loss 1.938, Val loss 1.931\n",
            "Epoch 1 (Step 000005): Train loss 1.631, Val loss 1.654\n",
            "Epoch 1 (Step 000010): Train loss 1.489, Val loss 1.502\n",
            "Epoch 1 (Step 000015): Train loss 1.417, Val loss 1.422\n",
            "Epoch 1 (Step 000020): Train loss 1.340, Val loss 1.431\n",
            "Epoch 1 (Step 000025): Train loss 1.337, Val loss 1.354\n",
            "Epoch 1 (Step 000030): Train loss 1.336, Val loss 1.342\n",
            "Epoch 1 (Step 000035): Train loss 1.312, Val loss 1.351\n",
            "Epoch 1 (Step 000040): Train loss 1.367, Val loss 1.321\n",
            "Epoch 1 (Step 000045): Train loss 1.308, Val loss 1.327\n",
            "Epoch 1 (Step 000050): Train loss 1.302, Val loss 1.319\n",
            "Epoch 1 (Step 000055): Train loss 1.301, Val loss 1.328\n",
            "Epoch 1 (Step 000060): Train loss 1.220, Val loss 1.334\n",
            "Epoch 1 (Step 000065): Train loss 1.303, Val loss 1.219\n",
            "Epoch 1 (Step 000070): Train loss 1.266, Val loss 1.236\n",
            "Epoch 1 (Step 000075): Train loss 1.242, Val loss 1.266\n",
            "Epoch 1 (Step 000080): Train loss 1.213, Val loss 1.278\n",
            "Epoch 1 (Step 000085): Train loss 1.194, Val loss 1.286\n",
            "Epoch 1 (Step 000090): Train loss 1.215, Val loss 1.272\n",
            "Epoch 1 (Step 000095): Train loss 1.236, Val loss 1.286\n",
            "explain who an outlier is? The The The   The The                                      \n",
            "Epoch 2 (Step 000100): Train loss 1.152, Val loss 1.297\n",
            "Epoch 2 (Step 000105): Train loss 1.214, Val loss 1.291\n",
            "Epoch 2 (Step 000110): Train loss 1.190, Val loss 1.238\n",
            "Epoch 2 (Step 000115): Train loss 1.180, Val loss 1.233\n",
            "Epoch 2 (Step 000120): Train loss 1.167, Val loss 1.141\n",
            "Epoch 2 (Step 000125): Train loss 1.168, Val loss 1.278\n",
            "Epoch 2 (Step 000130): Train loss 1.174, Val loss 1.205\n",
            "Epoch 2 (Step 000135): Train loss 1.086, Val loss 1.258\n",
            "Epoch 2 (Step 000140): Train loss 1.126, Val loss 1.190\n",
            "Epoch 2 (Step 000145): Train loss 1.151, Val loss 1.224\n",
            "Epoch 2 (Step 000150): Train loss 1.091, Val loss 1.135\n",
            "Epoch 2 (Step 000155): Train loss 1.148, Val loss 1.176\n",
            "Epoch 2 (Step 000160): Train loss 1.059, Val loss 1.145\n",
            "Epoch 2 (Step 000165): Train loss 1.096, Val loss 1.159\n",
            "Epoch 2 (Step 000170): Train loss 1.094, Val loss 1.197\n",
            "Epoch 2 (Step 000175): Train loss 1.102, Val loss 1.150\n",
            "Epoch 2 (Step 000180): Train loss 1.110, Val loss 1.130\n",
            "Epoch 2 (Step 000185): Train loss 1.059, Val loss 1.167\n",
            "Epoch 2 (Step 000190): Train loss 1.051, Val loss 1.142\n",
            "Epoch 2 (Step 000195): Train loss 1.022, Val loss 1.178\n",
            "explain who an outlier is? ’s a ’s a ’s a ’s a ’s a ’t have to ’s a ’s ’s ’t have to\n",
            "Epoch 3 (Step 000200): Train loss 1.036, Val loss 1.091\n",
            "Epoch 3 (Step 000205): Train loss 1.003, Val loss 1.201\n",
            "Epoch 3 (Step 000210): Train loss 1.021, Val loss 1.115\n",
            "Epoch 3 (Step 000215): Train loss 1.025, Val loss 1.090\n",
            "Epoch 3 (Step 000220): Train loss 0.997, Val loss 1.085\n",
            "Epoch 3 (Step 000225): Train loss 1.030, Val loss 1.058\n",
            "Epoch 3 (Step 000230): Train loss 0.976, Val loss 1.092\n",
            "Epoch 3 (Step 000235): Train loss 0.998, Val loss 1.134\n",
            "Epoch 3 (Step 000240): Train loss 0.958, Val loss 1.096\n",
            "Epoch 3 (Step 000245): Train loss 0.932, Val loss 1.070\n",
            "Epoch 3 (Step 000250): Train loss 0.939, Val loss 1.080\n",
            "Epoch 3 (Step 000255): Train loss 1.002, Val loss 1.076\n",
            "Epoch 3 (Step 000260): Train loss 0.925, Val loss 1.072\n",
            "Epoch 3 (Step 000265): Train loss 0.990, Val loss 1.102\n",
            "Epoch 3 (Step 000270): Train loss 0.945, Val loss 1.017\n",
            "Epoch 3 (Step 000275): Train loss 0.962, Val loss 1.040\n",
            "Epoch 3 (Step 000280): Train loss 0.961, Val loss 1.037\n",
            "Epoch 3 (Step 000285): Train loss 0.926, Val loss 1.092\n",
            "Epoch 3 (Step 000290): Train loss 0.933, Val loss 1.040\n",
            "Epoch 3 (Step 000295): Train loss 0.910, Val loss 1.051\n",
            "explain who an outlier is? the way of the the way of the the way of the the way of the the way of the the way of the the way of the the world have been the the way of the the way of\n",
            "Epoch 4 (Step 000300): Train loss 0.900, Val loss 1.048\n",
            "Epoch 4 (Step 000305): Train loss 0.870, Val loss 1.041\n",
            "Epoch 4 (Step 000310): Train loss 0.853, Val loss 1.063\n",
            "Epoch 4 (Step 000315): Train loss 0.899, Val loss 1.107\n",
            "Epoch 4 (Step 000320): Train loss 0.841, Val loss 1.052\n",
            "Epoch 4 (Step 000325): Train loss 0.865, Val loss 1.038\n",
            "Epoch 4 (Step 000330): Train loss 0.808, Val loss 1.111\n",
            "Epoch 4 (Step 000335): Train loss 0.822, Val loss 1.074\n",
            "Epoch 4 (Step 000340): Train loss 0.783, Val loss 1.039\n",
            "Epoch 4 (Step 000345): Train loss 0.846, Val loss 1.050\n",
            "Epoch 4 (Step 000350): Train loss 0.820, Val loss 1.036\n",
            "Epoch 4 (Step 000355): Train loss 0.850, Val loss 0.993\n",
            "Epoch 4 (Step 000360): Train loss 0.753, Val loss 1.015\n",
            "Epoch 4 (Step 000365): Train loss 0.771, Val loss 1.036\n",
            "Epoch 4 (Step 000370): Train loss 0.803, Val loss 1.017\n",
            "Epoch 4 (Step 000375): Train loss 0.777, Val loss 0.985\n",
            "Epoch 4 (Step 000380): Train loss 0.774, Val loss 1.089\n",
            "Epoch 4 (Step 000385): Train loss 0.778, Val loss 1.009\n",
            "Epoch 4 (Step 000390): Train loss 0.750, Val loss 0.974\n",
            "Epoch 4 (Step 000395): Train loss 0.785, Val loss 0.966\n",
            "explain who an outlier is? the way of the the top of the  the top of the  the “I’s a   the other words.  the “I” the fact that we’s not\n",
            "Epoch 5 (Step 000400): Train loss 0.740, Val loss 1.015\n",
            "Epoch 5 (Step 000405): Train loss 0.677, Val loss 0.977\n",
            "Epoch 5 (Step 000410): Train loss 0.743, Val loss 0.955\n",
            "Epoch 5 (Step 000415): Train loss 0.739, Val loss 1.053\n",
            "Epoch 5 (Step 000420): Train loss 0.723, Val loss 0.968\n",
            "Epoch 5 (Step 000425): Train loss 0.702, Val loss 0.948\n",
            "Epoch 5 (Step 000430): Train loss 0.686, Val loss 1.057\n",
            "Epoch 5 (Step 000435): Train loss 0.693, Val loss 0.937\n",
            "Epoch 5 (Step 000440): Train loss 0.600, Val loss 1.013\n",
            "Epoch 5 (Step 000445): Train loss 0.660, Val loss 0.897\n",
            "Epoch 5 (Step 000450): Train loss 0.644, Val loss 1.011\n",
            "Epoch 5 (Step 000455): Train loss 0.626, Val loss 0.956\n",
            "Epoch 5 (Step 000460): Train loss 0.654, Val loss 1.007\n",
            "Epoch 5 (Step 000465): Train loss 0.649, Val loss 0.900\n",
            "Epoch 5 (Step 000470): Train loss 0.594, Val loss 0.917\n",
            "Epoch 5 (Step 000475): Train loss 0.567, Val loss 0.961\n",
            "Epoch 5 (Step 000480): Train loss 0.569, Val loss 0.933\n",
            "Epoch 5 (Step 000485): Train loss 0.516, Val loss 0.919\n",
            "Epoch 5 (Step 000490): Train loss 0.558, Val loss 0.896\n",
            "Epoch 5 (Step 000495): Train loss 0.608, Val loss 0.939\n",
            "explain who an outlier is? and the  the way of the the “If you’s not the   The students who are the the way of the   the “I think, and the the way of the \n",
            "Epoch 6 (Step 000500): Train loss 0.563, Val loss 1.010\n",
            "Epoch 6 (Step 000505): Train loss 0.540, Val loss 1.003\n",
            "Epoch 6 (Step 000510): Train loss 0.565, Val loss 0.893\n",
            "Epoch 6 (Step 000515): Train loss 0.512, Val loss 0.919\n",
            "Epoch 6 (Step 000520): Train loss 0.535, Val loss 0.903\n",
            "Epoch 6 (Step 000525): Train loss 0.555, Val loss 0.996\n",
            "Epoch 6 (Step 000530): Train loss 0.452, Val loss 0.972\n",
            "Epoch 6 (Step 000535): Train loss 0.501, Val loss 0.964\n",
            "Epoch 6 (Step 000540): Train loss 0.440, Val loss 0.895\n",
            "Epoch 6 (Step 000545): Train loss 0.416, Val loss 0.835\n",
            "Epoch 6 (Step 000550): Train loss 0.511, Val loss 0.945\n",
            "Epoch 6 (Step 000555): Train loss 0.400, Val loss 0.844\n",
            "Epoch 6 (Step 000560): Train loss 0.458, Val loss 0.815\n",
            "Epoch 6 (Step 000565): Train loss 0.398, Val loss 0.830\n",
            "Epoch 6 (Step 000570): Train loss 0.409, Val loss 0.890\n",
            "Epoch 6 (Step 000575): Train loss 0.391, Val loss 0.788\n",
            "Epoch 6 (Step 000580): Train loss 0.429, Val loss 0.858\n",
            "Epoch 6 (Step 000585): Train loss 0.397, Val loss 0.774\n",
            "Epoch 6 (Step 000590): Train loss 0.368, Val loss 0.911\n",
            "Epoch 6 (Step 000595): Train loss 0.396, Val loss 0.886\n",
            "explain who an outlier is? She starts to the number of the the University of the best and students who is that the world is to his king-great-class children of the right side of the West and not a and-year-hour\n",
            "Epoch 7 (Step 000600): Train loss 0.412, Val loss 0.790\n",
            "Epoch 7 (Step 000605): Train loss 0.343, Val loss 0.817\n",
            "Epoch 7 (Step 000610): Train loss 0.337, Val loss 0.826\n",
            "Epoch 7 (Step 000615): Train loss 0.319, Val loss 0.806\n",
            "Epoch 7 (Step 000620): Train loss 0.383, Val loss 0.814\n",
            "Epoch 7 (Step 000625): Train loss 0.361, Val loss 0.811\n",
            "Epoch 7 (Step 000630): Train loss 0.278, Val loss 0.869\n",
            "Epoch 7 (Step 000635): Train loss 0.327, Val loss 0.886\n",
            "Epoch 7 (Step 000640): Train loss 0.287, Val loss 0.800\n",
            "Epoch 7 (Step 000645): Train loss 0.319, Val loss 0.716\n",
            "Epoch 7 (Step 000650): Train loss 0.306, Val loss 0.830\n",
            "Epoch 7 (Step 000655): Train loss 0.286, Val loss 0.877\n",
            "Epoch 7 (Step 000660): Train loss 0.269, Val loss 0.936\n",
            "Epoch 7 (Step 000665): Train loss 0.295, Val loss 0.821\n",
            "Epoch 7 (Step 000670): Train loss 0.280, Val loss 0.869\n",
            "Epoch 7 (Step 000675): Train loss 0.231, Val loss 0.816\n",
            "Epoch 7 (Step 000680): Train loss 0.248, Val loss 0.794\n",
            "Epoch 7 (Step 000685): Train loss 0.204, Val loss 0.843\n",
            "Epoch 7 (Step 000690): Train loss 0.284, Val loss 0.831\n",
            "Epoch 7 (Step 000695): Train loss 0.255, Val loss 0.715\n",
            "explain who an outlier is? —and not—and the world, we’ve been together by the the cockpit recorder and his wayhome from the  the way of the —and at the end of the Raven’s as much as exists of\n",
            "Epoch 8 (Step 000700): Train loss 0.213, Val loss 0.663\n",
            "Epoch 8 (Step 000705): Train loss 0.224, Val loss 0.717\n",
            "Epoch 8 (Step 000710): Train loss 0.196, Val loss 0.742\n",
            "Epoch 8 (Step 000715): Train loss 0.229, Val loss 0.783\n",
            "Epoch 8 (Step 000720): Train loss 0.245, Val loss 0.810\n",
            "Epoch 8 (Step 000725): Train loss 0.138, Val loss 0.734\n",
            "Epoch 8 (Step 000730): Train loss 0.213, Val loss 0.798\n",
            "Epoch 8 (Step 000735): Train loss 0.156, Val loss 0.812\n",
            "Epoch 8 (Step 000740): Train loss 0.171, Val loss 0.770\n",
            "Epoch 8 (Step 000745): Train loss 0.184, Val loss 0.782\n",
            "Epoch 8 (Step 000750): Train loss 0.195, Val loss 0.570\n",
            "Epoch 8 (Step 000755): Train loss 0.150, Val loss 0.764\n",
            "Epoch 8 (Step 000760): Train loss 0.146, Val loss 0.622\n",
            "Epoch 8 (Step 000765): Train loss 0.139, Val loss 0.740\n",
            "Epoch 8 (Step 000770): Train loss 0.214, Val loss 0.632\n",
            "Epoch 8 (Step 000775): Train loss 0.239, Val loss 0.691\n",
            "Epoch 8 (Step 000780): Train loss 0.194, Val loss 0.694\n",
            "Epoch 8 (Step 000785): Train loss 0.171, Val loss 0.697\n",
            "Epoch 8 (Step 000790): Train loss 0.119, Val loss 0.612\n",
            "Epoch 8 (Step 000795): Train loss 0.181, Val loss 0.674\n",
            "explain who an outlier is? good enough get put into a opportunity, are related to realize that all In the way fractions are expressed in the role preparation seems that what we succeed—itlement: it In the most of the way of the way\n",
            "Epoch 9 (Step 000800): Train loss 0.131, Val loss 0.732\n",
            "Epoch 9 (Step 000805): Train loss 0.166, Val loss 0.804\n",
            "Epoch 9 (Step 000810): Train loss 0.095, Val loss 0.713\n",
            "Epoch 9 (Step 000815): Train loss 0.126, Val loss 0.713\n",
            "Epoch 9 (Step 000820): Train loss 0.159, Val loss 0.825\n",
            "Epoch 9 (Step 000825): Train loss 0.101, Val loss 0.786\n",
            "Epoch 9 (Step 000830): Train loss 0.089, Val loss 0.693\n",
            "Epoch 9 (Step 000835): Train loss 0.115, Val loss 0.678\n",
            "Epoch 9 (Step 000840): Train loss 0.107, Val loss 0.583\n",
            "Epoch 9 (Step 000845): Train loss 0.101, Val loss 0.502\n",
            "Epoch 9 (Step 000850): Train loss 0.099, Val loss 0.646\n",
            "Epoch 9 (Step 000855): Train loss 0.099, Val loss 0.764\n",
            "Epoch 9 (Step 000860): Train loss 0.096, Val loss 0.721\n",
            "Epoch 9 (Step 000865): Train loss 0.074, Val loss 0.816\n",
            "Epoch 9 (Step 000870): Train loss 0.072, Val loss 0.687\n",
            "Epoch 9 (Step 000875): Train loss 0.061, Val loss 0.505\n",
            "Epoch 9 (Step 000880): Train loss 0.075, Val loss 0.722\n",
            "Epoch 9 (Step 000885): Train loss 0.071, Val loss 0.621\n",
            "Epoch 9 (Step 000890): Train loss 0.068, Val loss 0.619\n",
            "Epoch 9 (Step 000895): Train loss 0.050, Val loss 0.572\n",
            "explain who an outlier is? What if there opportunities to personal insult. But think outliers is a simple function of how deeply So why a good school example, to or above the result, to professional pianists. In general intelligence and \n",
            "Epoch 10 (Step 000900): Train loss 0.071, Val loss 0.595\n",
            "Epoch 10 (Step 000905): Train loss 0.072, Val loss 0.620\n",
            "Epoch 10 (Step 000910): Train loss 0.069, Val loss 0.502\n",
            "Epoch 10 (Step 000915): Train loss 0.053, Val loss 0.409\n",
            "Epoch 10 (Step 000920): Train loss 0.046, Val loss 0.678\n",
            "Epoch 10 (Step 000925): Train loss 0.052, Val loss 0.646\n",
            "Epoch 10 (Step 000930): Train loss 0.051, Val loss 0.607\n",
            "Epoch 10 (Step 000935): Train loss 0.043, Val loss 0.558\n",
            "Epoch 10 (Step 000940): Train loss 0.040, Val loss 0.437\n",
            "Epoch 10 (Step 000945): Train loss 0.058, Val loss 0.564\n",
            "Epoch 10 (Step 000950): Train loss 0.056, Val loss 0.631\n",
            "Epoch 10 (Step 000955): Train loss 0.037, Val loss 0.463\n",
            "Epoch 10 (Step 000960): Train loss 0.040, Val loss 0.581\n",
            "Epoch 10 (Step 000965): Train loss 0.038, Val loss 0.687\n",
            "Epoch 10 (Step 000970): Train loss 0.041, Val loss 0.577\n",
            "Epoch 10 (Step 000975): Train loss 0.033, Val loss 0.552\n",
            "Epoch 10 (Step 000980): Train loss 0.036, Val loss 0.589\n",
            "Epoch 10 (Step 000985): Train loss 0.035, Val loss 0.532\n",
            "Epoch 10 (Step 000990): Train loss 0.030, Val loss 0.492\n",
            "Epoch 10 (Step 000995): Train loss 0.035, Val loss 0.547\n",
            "explain who an outlier is? What if there opportunities to personal individual success? How does this rule alter our understanding of health,     OPPORTUNITY  CHAPTER ONE  In England, I know what we have far from \n",
            "Epoch 11 (Step 001000): Train loss 0.032, Val loss 0.714\n",
            "Epoch 11 (Step 001005): Train loss 0.029, Val loss 0.535\n",
            "Epoch 11 (Step 001010): Train loss 0.042, Val loss 0.577\n",
            "Epoch 11 (Step 001015): Train loss 0.028, Val loss 0.578\n",
            "Epoch 11 (Step 001020): Train loss 0.021, Val loss 0.360\n",
            "Epoch 11 (Step 001025): Train loss 0.033, Val loss 0.388\n",
            "Epoch 11 (Step 001030): Train loss 0.024, Val loss 0.448\n",
            "Epoch 11 (Step 001035): Train loss 0.021, Val loss 0.699\n",
            "Epoch 11 (Step 001040): Train loss 0.047, Val loss 0.626\n",
            "Epoch 11 (Step 001045): Train loss 0.022, Val loss 0.448\n",
            "Epoch 11 (Step 001050): Train loss 0.029, Val loss 0.543\n",
            "Epoch 11 (Step 001055): Train loss 0.024, Val loss 0.487\n",
            "Epoch 11 (Step 001060): Train loss 0.022, Val loss 0.444\n",
            "Epoch 11 (Step 001065): Train loss 0.023, Val loss 0.741\n",
            "Epoch 11 (Step 001070): Train loss 0.023, Val loss 0.447\n",
            "Epoch 11 (Step 001075): Train loss 0.020, Val loss 0.525\n",
            "Epoch 11 (Step 001080): Train loss 0.026, Val loss 0.476\n",
            "Epoch 11 (Step 001085): Train loss 0.012, Val loss 0.683\n",
            "Epoch 11 (Step 001090): Train loss 0.021, Val loss 0.800\n",
            "Epoch 11 (Step 001095): Train loss 0.017, Val loss 0.425\n",
            "explain who an outlier is? No, we’re going with a better school, the time, the first we have to sex to do better in the kind of time, the second, the chapters ahead, planes suggest something very different—that being good at\n",
            "Epoch 12 (Step 001100): Train loss 0.019, Val loss 0.710\n",
            "Epoch 12 (Step 001105): Train loss 0.015, Val loss 0.497\n",
            "Epoch 12 (Step 001110): Train loss 0.014, Val loss 0.503\n",
            "Epoch 12 (Step 001115): Train loss 0.013, Val loss 0.502\n",
            "Epoch 12 (Step 001120): Train loss 0.020, Val loss 0.494\n",
            "Epoch 12 (Step 001125): Train loss 0.017, Val loss 0.406\n",
            "Epoch 12 (Step 001130): Train loss 0.010, Val loss 0.716\n",
            "Epoch 12 (Step 001135): Train loss 0.011, Val loss 0.481\n",
            "Epoch 12 (Step 001140): Train loss 0.012, Val loss 0.600\n",
            "Epoch 12 (Step 001145): Train loss 0.018, Val loss 0.571\n",
            "Epoch 12 (Step 001150): Train loss 0.010, Val loss 0.276\n",
            "Epoch 12 (Step 001155): Train loss 0.010, Val loss 0.556\n",
            "Epoch 12 (Step 001160): Train loss 0.010, Val loss 0.689\n",
            "Epoch 12 (Step 001165): Train loss 0.011, Val loss 0.470\n",
            "Epoch 12 (Step 001170): Train loss 0.012, Val loss 0.595\n",
            "Epoch 12 (Step 001175): Train loss 0.011, Val loss 0.474\n",
            "Epoch 12 (Step 001180): Train loss 0.017, Val loss 0.408\n",
            "Epoch 12 (Step 001185): Train loss 0.014, Val loss 0.495\n",
            "Epoch 12 (Step 001190): Train loss 0.008, Val loss 0.464\n",
            "Epoch 12 (Step 001195): Train loss 0.008, Val loss 0.592\n",
            "explain who an outlier is? No as I noticed that the book is dedicated to “Daisy.” Who is she?  Daisy is my grandmother. She was a remarkable woman who was responsible for my mother’s success—for the fact\n",
            "Epoch 13 (Step 001200): Train loss 0.011, Val loss 0.353\n",
            "Epoch 13 (Step 001205): Train loss 0.009, Val loss 0.488\n",
            "Epoch 13 (Step 001210): Train loss 0.010, Val loss 0.481\n",
            "Epoch 13 (Step 001215): Train loss 0.008, Val loss 0.372\n",
            "Epoch 13 (Step 001220): Train loss 0.007, Val loss 0.595\n",
            "Epoch 13 (Step 001225): Train loss 0.008, Val loss 0.519\n",
            "Epoch 13 (Step 001230): Train loss 0.010, Val loss 0.377\n",
            "Epoch 13 (Step 001235): Train loss 0.006, Val loss 0.374\n",
            "Epoch 13 (Step 001240): Train loss 0.009, Val loss 0.557\n",
            "Epoch 13 (Step 001245): Train loss 0.010, Val loss 0.604\n",
            "Epoch 13 (Step 001250): Train loss 0.007, Val loss 0.415\n",
            "Epoch 13 (Step 001255): Train loss 0.008, Val loss 0.375\n",
            "Epoch 13 (Step 001260): Train loss 0.010, Val loss 0.553\n",
            "Epoch 13 (Step 001265): Train loss 0.006, Val loss 0.337\n",
            "Epoch 13 (Step 001270): Train loss 0.005, Val loss 0.483\n",
            "Epoch 13 (Step 001275): Train loss 0.010, Val loss 0.621\n",
            "Epoch 13 (Step 001280): Train loss 0.009, Val loss 0.440\n",
            "Epoch 13 (Step 001285): Train loss 0.004, Val loss 0.356\n",
            "Epoch 13 (Step 001290): Train loss 0.008, Val loss 0.470\n",
            "Epoch 13 (Step 001295): Train loss 0.011, Val loss 0.816\n",
            "explain who an outlier is? No, we have a better today don’t perform twelve hundred times in order to have to landings in order to describe what pilots relied the extended family clans that point that success: so what? Are choices or actions in\n",
            "Epoch 14 (Step 001300): Train loss 0.009, Val loss 0.504\n",
            "Epoch 14 (Step 001305): Train loss 0.007, Val loss 0.597\n",
            "Epoch 14 (Step 001310): Train loss 0.007, Val loss 0.604\n",
            "Epoch 14 (Step 001315): Train loss 0.006, Val loss 0.877\n",
            "Epoch 14 (Step 001320): Train loss 0.007, Val loss 0.692\n",
            "Epoch 14 (Step 001325): Train loss 0.006, Val loss 0.352\n",
            "Epoch 14 (Step 001330): Train loss 0.004, Val loss 0.441\n",
            "Epoch 14 (Step 001335): Train loss 0.005, Val loss 0.688\n",
            "Epoch 14 (Step 001340): Train loss 0.007, Val loss 0.411\n",
            "Epoch 14 (Step 001345): Train loss 0.006, Val loss 0.461\n",
            "Epoch 14 (Step 001350): Train loss 0.004, Val loss 0.486\n",
            "Epoch 14 (Step 001355): Train loss 0.006, Val loss 0.360\n",
            "Epoch 14 (Step 001360): Train loss 0.005, Val loss 0.510\n",
            "Epoch 14 (Step 001365): Train loss 0.005, Val loss 0.428\n",
            "Epoch 14 (Step 001370): Train loss 0.005, Val loss 0.435\n",
            "Epoch 14 (Step 001375): Train loss 0.003, Val loss 0.239\n",
            "Epoch 14 (Step 001380): Train loss 0.003, Val loss 0.701\n",
            "Epoch 14 (Step 001385): Train loss 0.007, Val loss 0.672\n",
            "Epoch 14 (Step 001390): Train loss 0.005, Val loss 0.401\n",
            "Epoch 14 (Step 001395): Train loss 0.005, Val loss 0.427\n",
            "explain who an outlier is? No, we have to have to plead their cases to authority. We have no idea That is in the lucky ones. It’re going to? But the sunlight that warmed them, the people in which they are the different\n",
            "Epoch 15 (Step 001400): Train loss 0.005, Val loss 0.511\n",
            "Epoch 15 (Step 001405): Train loss 0.004, Val loss 0.464\n",
            "Epoch 15 (Step 001410): Train loss 0.006, Val loss 0.775\n",
            "Epoch 15 (Step 001415): Train loss 0.003, Val loss 0.503\n",
            "Epoch 15 (Step 001420): Train loss 0.006, Val loss 0.229\n",
            "Epoch 15 (Step 001425): Train loss 0.006, Val loss 0.557\n",
            "Epoch 15 (Step 001430): Train loss 0.004, Val loss 0.239\n",
            "Epoch 15 (Step 001435): Train loss 0.005, Val loss 0.820\n",
            "Epoch 15 (Step 001440): Train loss 0.004, Val loss 0.397\n",
            "Epoch 15 (Step 001445): Train loss 0.005, Val loss 0.544\n",
            "Epoch 15 (Step 001450): Train loss 0.007, Val loss 0.712\n",
            "Epoch 15 (Step 001455): Train loss 0.004, Val loss 0.559\n",
            "Epoch 15 (Step 001460): Train loss 0.005, Val loss 0.336\n",
            "Epoch 15 (Step 001465): Train loss 0.004, Val loss 0.237\n",
            "Epoch 15 (Step 001470): Train loss 0.003, Val loss 0.563\n",
            "Epoch 15 (Step 001475): Train loss 0.004, Val loss 0.517\n",
            "Epoch 15 (Step 001480): Train loss 0.004, Val loss 0.124\n",
            "Epoch 15 (Step 001485): Train loss 0.004, Val loss 0.323\n",
            "Epoch 15 (Step 001490): Train loss 0.005, Val loss 0.375\n",
            "Epoch 15 (Step 001495): Train loss 0.004, Val loss 0.481\n",
            "explain who an outlier is? No, we have to care of the steady accumulation of advantages: when and where you are born, what your parents did for a living, and what the circumstances of your upbringing were all make a significant difference in how well you\n",
            "Epoch 16 (Step 001500): Train loss 0.005, Val loss 0.606\n",
            "Epoch 16 (Step 001505): Train loss 0.003, Val loss 0.480\n",
            "Epoch 16 (Step 001510): Train loss 0.006, Val loss 0.297\n",
            "Epoch 16 (Step 001515): Train loss 0.004, Val loss 0.415\n",
            "Epoch 16 (Step 001520): Train loss 0.005, Val loss 0.823\n",
            "Epoch 16 (Step 001525): Train loss 0.002, Val loss 0.654\n",
            "Epoch 16 (Step 001530): Train loss 0.004, Val loss 0.636\n",
            "Epoch 16 (Step 001535): Train loss 0.004, Val loss 0.345\n",
            "Epoch 16 (Step 001540): Train loss 0.005, Val loss 0.451\n",
            "Epoch 16 (Step 001545): Train loss 0.003, Val loss 0.546\n",
            "Epoch 16 (Step 001550): Train loss 0.003, Val loss 0.276\n",
            "Epoch 16 (Step 001555): Train loss 0.004, Val loss 0.236\n",
            "Epoch 16 (Step 001560): Train loss 0.004, Val loss 0.641\n",
            "Epoch 16 (Step 001565): Train loss 0.003, Val loss 0.395\n",
            "Epoch 16 (Step 001570): Train loss 0.003, Val loss 0.654\n",
            "Epoch 16 (Step 001575): Train loss 0.003, Val loss 0.208\n",
            "Epoch 16 (Step 001580): Train loss 0.003, Val loss 0.116\n",
            "Epoch 16 (Step 001585): Train loss 0.003, Val loss 0.528\n",
            "Epoch 16 (Step 001590): Train loss 0.005, Val loss 0.757\n",
            "Epoch 16 (Step 001595): Train loss 0.003, Val loss 0.518\n",
            "explain who an outlier is? No, we have to have to plead their cases to have to procedure regardless of circumstances: we know a kind of course, can we’ve achieved parents as something separate from, captains and the logic behind who succeeds extra\n",
            "Epoch 17 (Step 001600): Train loss 0.003, Val loss 0.293\n",
            "Epoch 17 (Step 001605): Train loss 0.003, Val loss 0.111\n",
            "Epoch 17 (Step 001610): Train loss 0.003, Val loss 0.447\n",
            "Epoch 17 (Step 001615): Train loss 0.004, Val loss 0.510\n",
            "Epoch 17 (Step 001620): Train loss 0.003, Val loss 0.471\n",
            "Epoch 17 (Step 001625): Train loss 0.004, Val loss 0.460\n",
            "Epoch 17 (Step 001630): Train loss 0.003, Val loss 0.680\n",
            "Epoch 17 (Step 001635): Train loss 0.004, Val loss 0.429\n",
            "Epoch 17 (Step 001640): Train loss 0.003, Val loss 0.339\n",
            "Epoch 17 (Step 001645): Train loss 0.003, Val loss 0.213\n",
            "Epoch 17 (Step 001650): Train loss 0.005, Val loss 0.543\n",
            "Epoch 17 (Step 001655): Train loss 0.004, Val loss 0.436\n",
            "Epoch 17 (Step 001660): Train loss 0.003, Val loss 0.665\n",
            "Epoch 17 (Step 001665): Train loss 0.003, Val loss 0.425\n",
            "Epoch 17 (Step 001670): Train loss 0.003, Val loss 0.659\n",
            "Epoch 17 (Step 001675): Train loss 0.003, Val loss 0.531\n",
            "Epoch 17 (Step 001680): Train loss 0.004, Val loss 0.630\n",
            "Epoch 17 (Step 001685): Train loss 0.002, Val loss 0.564\n",
            "Epoch 17 (Step 001690): Train loss 0.002, Val loss 0.323\n",
            "Epoch 17 (Step 001695): Train loss 0.004, Val loss 0.592\n",
            "explain who an outlier is? No, we’ve even though personal insult. I realize that it seems strange to refer to American Jewish immigrants as lucky when the families and relatives they left behind in Europe were on the verge of extermination at the hands of the\n",
            "Epoch 18 (Step 001700): Train loss 0.003, Val loss 0.424\n",
            "Epoch 18 (Step 001705): Train loss 0.003, Val loss 0.321\n",
            "Epoch 18 (Step 001710): Train loss 0.003, Val loss 0.213\n",
            "Epoch 18 (Step 001715): Train loss 0.002, Val loss 0.239\n",
            "Epoch 18 (Step 001720): Train loss 0.003, Val loss 0.229\n",
            "Epoch 18 (Step 001725): Train loss 0.005, Val loss 0.219\n",
            "Epoch 18 (Step 001730): Train loss 0.003, Val loss 0.748\n",
            "Epoch 18 (Step 001735): Train loss 0.002, Val loss 0.646\n",
            "Epoch 18 (Step 001740): Train loss 0.003, Val loss 0.224\n",
            "Epoch 18 (Step 001745): Train loss 0.002, Val loss 0.700\n",
            "Epoch 18 (Step 001750): Train loss 0.004, Val loss 0.483\n",
            "Epoch 18 (Step 001755): Train loss 0.002, Val loss 0.445\n",
            "Epoch 18 (Step 001760): Train loss 0.005, Val loss 0.404\n",
            "Epoch 18 (Step 001765): Train loss 0.002, Val loss 0.231\n",
            "Epoch 18 (Step 001770): Train loss 0.003, Val loss 0.431\n",
            "Epoch 18 (Step 001775): Train loss 0.002, Val loss 0.423\n",
            "Epoch 18 (Step 001780): Train loss 0.004, Val loss 0.211\n",
            "Epoch 18 (Step 001785): Train loss 0.003, Val loss 0.633\n",
            "Epoch 18 (Step 001790): Train loss 0.002, Val loss 0.319\n",
            "Epoch 18 (Step 001795): Train loss 0.002, Val loss 0.416\n",
            "explain who an outlier is? No, we’re going with a better school, do is a culture, with what successful people are like, at long last, so are the course, or biographies of us have a mind so far, or a extra\n",
            "Epoch 19 (Step 001800): Train loss 0.002, Val loss 0.418\n",
            "Epoch 19 (Step 001805): Train loss 0.003, Val loss 0.198\n",
            "Epoch 19 (Step 001810): Train loss 0.002, Val loss 0.425\n",
            "Epoch 19 (Step 001815): Train loss 0.003, Val loss 0.430\n",
            "Epoch 19 (Step 001820): Train loss 0.003, Val loss 0.528\n",
            "Epoch 19 (Step 001825): Train loss 0.002, Val loss 0.435\n",
            "Epoch 19 (Step 001830): Train loss 0.004, Val loss 0.436\n",
            "Epoch 19 (Step 001835): Train loss 0.002, Val loss 0.317\n",
            "Epoch 19 (Step 001840): Train loss 0.002, Val loss 0.471\n",
            "Epoch 19 (Step 001845): Train loss 0.004, Val loss 0.326\n",
            "Epoch 19 (Step 001850): Train loss 0.002, Val loss 0.312\n",
            "Epoch 19 (Step 001855): Train loss 0.003, Val loss 0.406\n",
            "Epoch 19 (Step 001860): Train loss 0.003, Val loss 0.527\n",
            "Epoch 19 (Step 001865): Train loss 0.003, Val loss 0.336\n",
            "Epoch 19 (Step 001870): Train loss 0.003, Val loss 0.528\n",
            "Epoch 19 (Step 001875): Train loss 0.002, Val loss 0.541\n",
            "Epoch 19 (Step 001880): Train loss 0.002, Val loss 0.327\n",
            "Epoch 19 (Step 001885): Train loss 0.004, Val loss 0.631\n",
            "Epoch 19 (Step 001890): Train loss 0.002, Val loss 0.605\n",
            "Epoch 19 (Step 001895): Train loss 0.002, Val loss 0.319\n",
            "explain who an outlier is? No, are opportunities to understand his scholarship at Reed, and work ethic. It’s something, are out of Chris Langan. students of Harvard appears on the him past the typical much of his discussion\n",
            "Epoch 20 (Step 001900): Train loss 0.002, Val loss 0.563\n",
            "Epoch 20 (Step 001905): Train loss 0.003, Val loss 0.425\n",
            "Epoch 20 (Step 001910): Train loss 0.003, Val loss 0.286\n",
            "Epoch 20 (Step 001915): Train loss 0.003, Val loss 0.334\n",
            "Epoch 20 (Step 001920): Train loss 0.002, Val loss 0.427\n",
            "Epoch 20 (Step 001925): Train loss 0.003, Val loss 0.724\n",
            "Epoch 20 (Step 001930): Train loss 0.003, Val loss 0.629\n",
            "Epoch 20 (Step 001935): Train loss 0.001, Val loss 0.316\n",
            "Epoch 20 (Step 001940): Train loss 0.002, Val loss 0.192\n",
            "Epoch 20 (Step 001945): Train loss 0.002, Val loss 0.641\n",
            "Epoch 20 (Step 001950): Train loss 0.003, Val loss 0.546\n",
            "Epoch 20 (Step 001955): Train loss 0.003, Val loss 0.628\n",
            "Epoch 20 (Step 001960): Train loss 0.002, Val loss 0.489\n",
            "Epoch 20 (Step 001965): Train loss 0.005, Val loss 0.341\n",
            "Epoch 20 (Step 001970): Train loss 0.004, Val loss 0.546\n",
            "Epoch 20 (Step 001975): Train loss 0.003, Val loss 0.677\n",
            "Epoch 20 (Step 001980): Train loss 0.002, Val loss 0.199\n",
            "Epoch 20 (Step 001985): Train loss 0.003, Val loss 0.487\n",
            "Epoch 20 (Step 001990): Train loss 0.004, Val loss 0.315\n",
            "Epoch 20 (Step 001995): Train loss 0.002, Val loss 0.427\n",
            "explain who an outlier is? No, are opportunities that Poole’s work. I don’t as a better than any That last statement may seem a remarkable woman who might think of famous people, I am heavily indebted. culture.\n",
            "training completed in 20.42 minutes.\n"
          ]
        }
      ],
      "source": [
        "# start timer for training duration.\n",
        "start_time = time.time()\n",
        "\n",
        "# set seed for reproducibility.\n",
        "torch.manual_seed(66)\n",
        "\n",
        "# initialize gpt model.\n",
        "model = GPTModel(gpt_config_124M)\n",
        "\n",
        "# move model to device.\n",
        "model.to(device)\n",
        "\n",
        "# set learning rate.\n",
        "learning_rate = 0.0004\n",
        "\n",
        "# set weight decay for regularization.\n",
        "weight_decay = 0.1\n",
        "\n",
        "# initialize adamw optimizer.\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# set number of training epochs.\n",
        "num_epochs = 20\n",
        "\n",
        "# train model and track losses.\n",
        "train_losses, val_losses, tokens_seen = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs=num_epochs,\n",
        "    eval_freq=5,\n",
        "    eval_iter=5,\n",
        "    starting_prompt=\"explain who an outlier is?\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# calculate and display training duration.\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Save the trained model**"
      ],
      "metadata": {
        "id": "7Ku103eafsRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of epochs.\n",
        "epochs = 20\n",
        "\n",
        "# create models directory if it doesn't exist.\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "def save_model(model, filepath):\n",
        "    \"\"\"saves final trained model to disk.\"\"\"\n",
        "\n",
        "    # create directory if it doesn't exist.\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "    # save only model state dict.\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "    print(f\"trained model saved at: {filepath}\")\n",
        "\n",
        "\n",
        "# define the path to save the model.\n",
        "model_path = f\"/content/drive/MyDrive/models/gpt2_final_epoch_{epochs}.pt\"\n",
        "\n",
        "# save final model (for inference only).\n",
        "save_model(model, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXtm7RTgfoK1",
        "outputId": "30c1be94-1b97-497a-f870-2f12bb174351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trained model saved at: /content/drive/MyDrive/models/gpt2_final_epoch_20.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the checkpoint of the trained model**"
      ],
      "metadata": {
        "id": "77ZCjD4ds9em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
        "    \"\"\"saves model checkpoint to disk.\"\"\"\n",
        "\n",
        "    # create checkpoint directory if it doesn't exist.\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "    # save checkpoint.\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"checkpoint saved: {filepath}\")\n",
        "\n",
        "\n",
        "# set number of epochs.\n",
        "epochs = 20\n",
        "\n",
        "# save final checkpoint.\n",
        "checkpoint_path = f\"/content/drive/MyDrive/checkpoint/checkpoint_epoch_{epochs}.pt\"\n",
        "\n",
        "# save model checkpoint to disk.\n",
        "save_checkpoint(model, optimizer, epochs, train_losses[-1], checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vttoxSY1gbXd",
        "outputId": "18ed5606-90d6-4a52-8076-912689681175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint saved: /content/drive/MyDrive/checkpoint/checkpoint_epoch_20.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load the trained Model**"
      ],
      "metadata": {
        "id": "6m8xEESsfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, filepath):\n",
        "    \"\"\"loads model checkpoint from disk.\"\"\"\n",
        "\n",
        "    # check if checkpoint exists.\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"checkpoint not found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "    # load checkpoint.\n",
        "    checkpoint = torch.load(filepath)\n",
        "\n",
        "    # load model and optimizer states.\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # extract the epoch and loss value.\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "    print(f\"checkpoint loaded: {filepath}\")\n",
        "    print(f\"resumed from epoch {epoch} with loss {loss:.4f}\")\n",
        "\n",
        "    return epoch, loss\n",
        "\n",
        "\n",
        "# initialize model and optimizer first.\n",
        "model = GPTModel(gpt_config_124M)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004)\n",
        "\n",
        "# load checkpoint.\n",
        "checkpoint_path = f\"/content/drive/MyDrive/checkpoint/checkpoint_epoch_{epochs}.pt\"\n",
        "epoch, loss = load_checkpoint(model, optimizer, checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHl-GatXkM-b",
        "outputId": "c439cf76-5492-45a3-e2a8-c1925d508907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint loaded: /content/drive/MyDrive/checkpoint/checkpoint_epoch_20.pt\n",
            "resumed from epoch 20 with loss 0.0020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model, filepath, device='cpu'):\n",
        "    \"\"\"loads trained model from disk.\"\"\"\n",
        "\n",
        "    # check if model file exists.\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"model not found: {filepath}\")\n",
        "        return None\n",
        "\n",
        "    # load model state dict.\n",
        "    model.load_state_dict(torch.load(filepath, map_location=device))\n",
        "\n",
        "    print(f\"model loaded from: {filepath}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hYZ7tgWPs7YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the trained model (which should be used for inference only).\n",
        "model_path = f\"/content/drive/MyDrive/models/gpt2_final_epoch_{epochs}.pt\"\n",
        "\n",
        "model = load_model(model, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZgs-VYhtJeU",
        "outputId": "7499b2e3-b80e-4abc-91c6-e1b695fa3bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded from: /content/drive/MyDrive/models/gpt2_final_epoch_20.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the Model for Word Predictions"
      ],
      "metadata": {
        "id": "9A72oIcmkNUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cpu\""
      ],
      "metadata": {
        "id": "2m_o31IBwkWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aixM5Zz3HuVX",
        "outputId": "8c9b4347-8598-4300-d45f-62aa43dc1452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output text:\n",
            " lawyer is a ‘dignified road tostarvation.’ ” Regardless of the number of years they had spent in practice, theirincome was “strikingly less” than that of their Christian colleagues. Maurice Janklowwas born in 1902. When the Depression started, he was newly married and had justbought his big car, moved to Queens, and made his great gamble on the writing-paperbusiness. His timing could not have been worse.\n",
            "\n",
            "“\n"
          ]
        }
      ],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    \"\"\"converts text to token ids with batch dimension.\"\"\"\n",
        "\n",
        "    # encode text to token ids.\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    # convert to tensor and add batch dimension.\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    \"\"\"converts token ids back to text.\"\"\"\n",
        "\n",
        "    # remove batch dimension.\n",
        "    flat = token_ids.squeeze(0)\n",
        "\n",
        "    # decode token ids to text.\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "# set starting prompt for text generation.\n",
        "prompt = \"lawyer is\"\n",
        "\n",
        "# convert text to token ids and move to device.\n",
        "input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n",
        "\n",
        "# generate text from prompt.\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=input_ids,\n",
        "    max_new_tokens=100,\n",
        "    context_size=gpt_config_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# display generated text.\n",
        "print(\"output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFOuHoOxXQWw"
      },
      "source": [
        "The model has been trained from scratch effectively to understand and capture the intricacies behind languages. Currently, it is somewhat excellent at predicting the next words (tokens). For improved performance in question-answering, it is advisable and recommendable to perform instruction fine-tuning on a massive instruction dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "ZpoVBL6QXPtd",
        "outputId": "c9c2a688-d1c5-45d0-ce90-a5b768406892"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def plot_metric(train_losses, val_losses, tokens_seen, title=\"loss per epoch\", save_path=None):\n",
        "    \"\"\"plots training and validation losses over tokens seen.\"\"\"\n",
        "\n",
        "    # create figure and axis.\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # plot training loss.\n",
        "    ax.plot(tokens_seen, train_losses, label=\"training loss\", color=\"blue\", linewidth=2, marker=\"o\")\n",
        "\n",
        "    # plot validation loss.\n",
        "    ax.plot(tokens_seen, val_losses, label=\"validation loss\", color=\"orange\", linewidth=2, marker=\"s\")\n",
        "\n",
        "    # add labels and title.\n",
        "    ax.set_xlabel(\"tokens seen\", fontsize=12)\n",
        "    ax.set_ylabel(\"loss\", fontsize=12)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "\n",
        "    # add legend.\n",
        "    ax.legend(fontsize=11)\n",
        "\n",
        "    # add grid for readability.\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "    # adjust layout.\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # save figure if path provided.\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"plot saved to: {save_path}\")\n",
        "\n",
        "    # display plot.\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_metric(train_losses, val_losses, tokens_seen)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1sji4hxt1wXl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PkfGYmvaa-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc19170-d69e-493b-cd75-af2b4e021c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded checkpoint from: /content/drive/MyDrive/checkpoint/checkpoint_epoch_20.pt\n",
            "\n",
            "prompt: a proverb related to the japanese says: no one who can rise before dawn three hundred\n",
            "\n",
            "generated text:\n",
            "a proverb related to the japanese says: no one who can rise before dawn three hundred safe himself to help pay his sentences. And without any financial aid either,’t working from—and prayed—New Scientist Berkeley. He has writtenonder how he once. interests study gets interesting,\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, tokenizer, prompt, max_tokens=100, temperature=1.0, device=\"cpu\"):\n",
        "    \"\"\"generates text from a prompt using the trained model.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # encode prompt.\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    input_tensor = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "    generated_ids = input_ids.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_tokens):\n",
        "\n",
        "            # get model predictions.\n",
        "            outputs = model(input_tensor)\n",
        "\n",
        "            # get last token logits.\n",
        "            logits = outputs[0, -1, :] / temperature\n",
        "\n",
        "            # sample next token.\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            # append to generated sequence.\n",
        "            generated_ids.append(next_token)\n",
        "\n",
        "            # update input tensor.\n",
        "            input_tensor = torch.tensor([generated_ids]).to(device)\n",
        "\n",
        "\n",
        "    # decode generated ids.\n",
        "    generated_text = tokenizer.decode(generated_ids)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "checkpoint = ''\n",
        "prompt = \"a proverb related to the japanese says: no one who can rise before dawn three hundred\"\n",
        "\n",
        "max_tokens = 45\n",
        "\n",
        "temperature = 1.0\n",
        "\n",
        "# initialize model.\n",
        "model = GPTModel(gpt_config_124M).to(device)\n",
        "\n",
        "# initialize optimizer.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004)\n",
        "\n",
        "# load checkpoint.\n",
        "checkpoint_path = f\"/content/drive/MyDrive/checkpoint/checkpoint_epoch_{epochs}.pt\"\n",
        "\n",
        "# load checkpoint.\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(f\"loaded checkpoint from: {checkpoint_path}\\n\")\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "\n",
        "# generate text using the trained model.\n",
        "generated_text = generate_text(model, tokenizer, prompt, max_tokens=max_tokens,\n",
        "                               temperature=temperature, device=device)\n",
        "\n",
        "# print generated text with newlines replaced by spaces.\n",
        "print(f\"generated text:\\n{generated_text.replace(\"\\n\", \" \")}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}