{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wra-Jzl4-KNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kgb8HuzI-KKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade pypdf tiktoken==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUQAzzLS-JmF",
        "outputId": "50afff84-dfd9-4ad4-de86-a97054c1de0c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rQHwnpm4e2M_"
      },
      "outputs": [],
      "source": [
        "# install all the required libraries, modules and packages for this project.\n",
        "import os\n",
        "from pypdf import PdfReader\n",
        "import re\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kBEoMXf194Cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f13abe-10c1-4396-c9b8-9ef39647c3af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/MyDrive/Datasets/Book/Outliers_Malcolm_Gladwell.pdf\""
      ],
      "metadata": {
        "id": "CAr9iyFJ96Lp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/MyDrive/Datasets/Book/\""
      ],
      "metadata": {
        "id": "dEwUWJo13sLe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_text(pdf_path, output_txt_path=None, save_to_file=True):\n",
        "    \"\"\"reads pdf and converts to text, optionally saving to .txt file.\"\"\"\n",
        "\n",
        "    # check if pdf exists.\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"error: '{pdf_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # open pdf in binary mode.\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "\n",
        "            # create pdf reader.\n",
        "            reader = PdfReader(file)\n",
        "            num_pages = len(reader.pages)\n",
        "            print(f\"processing {num_pages} pages...\")\n",
        "\n",
        "            # extract all text.\n",
        "            full_text = []\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "\n",
        "                if text:\n",
        "                    full_text.append(text)\n",
        "                else:\n",
        "                    print(f\"warning: page {page_num + 1} had no extractable text.\")\n",
        "\n",
        "            # combine all pages.\n",
        "            combined_text = \"\\n\\n\".join(full_text)\n",
        "\n",
        "            # save to file if requested.\n",
        "            if save_to_file:\n",
        "                if output_txt_path is None:\n",
        "                    output_txt_path = pdf_path.replace('.pdf', '.txt')\n",
        "\n",
        "                with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "                    txt_file.write(combined_text)\n",
        "\n",
        "                print(f\"saved to: {output_txt_path}\")\n",
        "\n",
        "            return combined_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error reading pdf: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "RHMIjhBS96IO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_pdf_to_text(folder_path, output_folder=None):\n",
        "    \"\"\"converts all pdfs in a folder to text files.\"\"\"\n",
        "\n",
        "    # check if folder exists.\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"error: folder '{folder_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    # create output folder if needed.\n",
        "    if output_folder and not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # get all pdf files.\n",
        "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"no pdf files found in folder.\")\n",
        "        return\n",
        "\n",
        "    print(f\"found {len(pdf_files)} pdf files. converting...\")\n",
        "\n",
        "    combined_texts = []\n",
        "\n",
        "    # process each pdf.\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(folder_path, pdf_file)\n",
        "\n",
        "        if output_folder:\n",
        "            output_path = os.path.join(output_folder, pdf_file.replace('.pdf', '.txt'))\n",
        "        else:\n",
        "            output_path = None\n",
        "\n",
        "        print(f\"\\nprocessing: {pdf_file}\")\n",
        "\n",
        "        combined_texts.append(pdf_to_text(pdf_path, output_path))\n",
        "\n",
        "    print(\"\\nbatch conversion complete!\")\n",
        "\n",
        "    return combined_texts"
      ],
      "metadata": {
        "id": "YHwQb_Og96E5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_contents = batch_pdf_to_text(pdf_path)"
      ],
      "metadata": {
        "id": "M8CBfx4u-C0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccffa370-5019-47a8-b3aa-b4c27ea752a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 1 pdf files. converting...\n",
            "\n",
            "processing: Outliers_Malcolm_Gladwell.pdf\n",
            "processing 249 pages...\n",
            "warning: page 1 had no extractable text.\n",
            "saved to: /content/drive/MyDrive/Datasets/Book/Outliers_Malcolm_Gladwell.txt\n",
            "\n",
            "batch conversion complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_content = raw_text_contents[0]\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text_content), \"\\n\")\n",
        "print(raw_text_content[:99])"
      ],
      "metadata": {
        "id": "ZKcxuA23-FWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "212003ec-85c8-467b-dca4-f18df5277cb6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 460987 \n",
            "\n",
            "OUTLIERS\n",
            "The Story of Success\n",
            "MALCOLM GLADWELL\n",
            "BACK BAY BOOKS\n",
            "LITTLE, BROWN AND COMPANY\n",
            "NEW YORK   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jNS4CqXw-U_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(raw_text, max_preview=30):\n",
        "    \"\"\"splits text on punctuation and whitespace, removes empty strings.\"\"\"\n",
        "\n",
        "    # split on common punctuation and whitespace.\n",
        "    pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    tokens = re.split(pattern, raw_text)\n",
        "\n",
        "    # remove whitespace and filter empty strings.\n",
        "    preprocessed = [item.strip() for item in tokens if item.strip()]\n",
        "\n",
        "    # preview first n tokens.\n",
        "    if max_preview:\n",
        "        print(f\"first {max_preview} tokens:\")\n",
        "        print(preprocessed[:max_preview])\n",
        "        print(f\"\\ntotal word level tokens: {len(preprocessed)}\\n\")\n",
        "\n",
        "    return preprocessed"
      ],
      "metadata": {
        "id": "iSChh9Pa-UNp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = preprocess_text(raw_text_content, max_preview=30)\n",
        "\n",
        "# get the vocabulary size of the dataset.\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "CWZsW8Gf-X7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6b4fce-e0e0-4c36-e8fc-9ddabe97c683"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 30 tokens:\n",
            "['OUTLIERS', 'The', 'Story', 'of', 'Success', 'MALCOLM', 'GLADWELL', 'BACK', 'BAY', 'BOOKS', 'LITTLE', ',', 'BROWN', 'AND', 'COMPANY', 'NEW', 'YORK', '•', 'BOSTON', '•', 'LONDON', 'Begin', 'Reading', 'Table', 'of', 'Contents', 'Reading', 'Group', 'Guide', 'Copyright']\n",
            "\n",
            "total word level tokens: 92758\n",
            "\n",
            "11324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "mNCA5ui1-aQv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jqV6_7Fc-cEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordTokenizer:\n",
        "    \"\"\"tokenizes text into ids and decodes ids back to text. it focuses on word-level tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"initializes tokenizer with vocabulary mappings.\"\"\"\n",
        "\n",
        "        self.tok_to_int = vocab\n",
        "        self.int_to_tok = {integer: token for token, integer in vocab.items()}\n",
        "        self.pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"converts text to list of token ids.\"\"\"\n",
        "\n",
        "        # split on punctuation and whitespace.\n",
        "        preprocessed = re.split(self.pattern, text)\n",
        "\n",
        "        # remove empty strings and whitespace.\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "        # replace unknown tokens with <|unk|>.\n",
        "        preprocessed = [token if token in self.tok_to_int else \"<|unk|>\" for token in preprocessed]\n",
        "\n",
        "        # convert tokens to ids.\n",
        "        ids = [self.tok_to_int[tok] for tok in preprocessed]\n",
        "\n",
        "        # return ids\n",
        "        return ids\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"converts list of token ids back to text.\"\"\"\n",
        "\n",
        "        # map ids to tokens.\n",
        "        tokens = [self.int_to_tok[id] for id in ids]\n",
        "\n",
        "        # join tokens with spaces.\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(self.pattern, r'\\1', text)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(r'\\s+([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-])', r'\\1', text)\n",
        "\n",
        "        return text"
      ],
      "metadata": {
        "id": "yNrHerbU-cyE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the wordtokenizer class.\n",
        "tokenizer = WordTokenizer(vocab)\n",
        "\n",
        "# sample text from (and outside) outliers book.\n",
        "text1 = \"Chris Langan's mother was from San Francisco and was estranged from her family.\"\n",
        "\n",
        "# sample text with unknown words for testing.\n",
        "text2 = \"do you know about smartphone cryptocurrency?\"\n",
        "\n",
        "\n",
        "# combine texts with special separator token.\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(f\"original text: {text}\")\n",
        "\n",
        "# convert text to token ids.\n",
        "encoded_text = tokenizer.encode(text)\n",
        "print(f\"\\nencoded text: {encoded_text}\")\n",
        "\n",
        "# convert token ids back to text.\n",
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(f\"\\ndecoded text: {decoded_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQjJIrkz6Szm",
        "outputId": "d9c9d187-17ed-4430-ce2d-b40f0c5196a5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: Chris Langan's mother was from San Francisco and was estranged from her family. <|endoftext|> do you know about smartphone cryptocurrency?\n",
            "\n",
            "encoded text: [915, 1905, 11325, 11325, 7500, 10581, 5993, 2676, 1342, 3561, 10581, 5524, 5993, 6370, 5700, 29, 11325, 11325, 11325, 11325, 11325, 5192, 10835, 6940, 3291, 11325, 11325, 430]\n",
            "\n",
            "decoded text: Chris Langan <|unk|> <|unk|> mother was from San Francisco and was estranged from her family. <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> do you know about <|unk|> <|unk|>?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fFfPOiXj6Wgx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOEmQlXQ6YbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}