{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wra-Jzl4-KNs"
      },
      "source": [
        "# **Tokenization for Generative Pretrained Transfromers (GPTs)**\n",
        "\n",
        "This notebook covers the text tokenization techniques used in Generative Pretrained Transformers (GPTs) - specifically GPT-2 and GPT-3 - model training.\n",
        "\n",
        "---\n",
        "\n",
        "## **Table of Contents**\n",
        "1. [Introduction](#introduction)\n",
        "2. [Loading Raw Text](#loading-raw-text)\n",
        "3. [Word-Level Tokenization](#word-level-tokenization)\n",
        "4. [Byte Pair Encoding (BPE)](#byte-pair-encoding-bpe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgb8HuzI-KKq"
      },
      "source": [
        "**Install all the required libraries or packages.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BUQAzzLS-JmF"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade pypdf tiktoken==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rQHwnpm4e2M_"
      },
      "outputs": [],
      "source": [
        "# import all the required libraries, modules and packages for this project.\n",
        "import os\n",
        "from pypdf import PdfReader\n",
        "import re\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBEoMXf194Cb",
        "outputId": "88f13abe-10c1-4396-c9b8-9ef39647c3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab0wuekGAavt"
      },
      "source": [
        "<a id=\"introduction\"></a>\n",
        "## **Introduction**\n",
        "\n",
        "In simple terms, **Tokenization** is the process of converting raw text (which could be internet or web data, books and any other textual contents or documents) into numerical sequences that machines (in this scenario, we mean computers) can read and understand, and neural networks can process for their training. In this notebook, we explored two tokenization approaches:\n",
        "\n",
        "- **Word-Level Tokenization**: This tokenization approach splits text into words and punctuation. It is the simplest type of tokenization.\n",
        "- **Byte Pair Encoding (BPE)**: This is the type of tokenization where texts are split into subword units for better compression and handling of rare or uncommon words that are not present in the training dataset or documents. It is the most efficient, effective and highly accurate tokenization technique, and it is used in most or every of today's leading Large Language Models (LLMs) - OpenAI GPT Family, Meta Llama Family and many others.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKRsaZ6LEiZx"
      },
      "source": [
        "<a id=\"loading-raw-text\"></a>\n",
        "## **Loading Raw Text**\n",
        "\n",
        "First, we load our raw text data from pdf file format and convert it to .txt file. Before any model can be trained, data must be provided during its learning or training stage process. The data here is the raw text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dEwUWJo13sLe"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"/content/drive/MyDrive/Datasets/Book/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajCkLgZ4Mo5T"
      },
      "source": [
        "### **PDF to Text Conversion**\n",
        "\n",
        "Before we can tokenize text, we need to extract it from PDF files. We've created two utility functions:\n",
        "\n",
        "- **`pdf_to_text()`**: Extracts text from a single PDF file and optionally saves it as a `.txt` file\n",
        "- **`batch_pdf_to_text()`**: Processes multiple PDFs in a folder at once, converting each to text\n",
        "\n",
        "These functions use the `pypdf` library to read PDFs page-by-page and combine the extracted text into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RHMIjhBS96IO"
      },
      "outputs": [],
      "source": [
        "def pdf_to_text(pdf_path, output_txt_path=None, save_to_file=True):\n",
        "    \"\"\"reads pdf and converts to text, optionally saving to .txt file.\"\"\"\n",
        "\n",
        "    # check if pdf exists.\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"error: '{pdf_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # open pdf in binary mode.\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "\n",
        "            # create pdf reader.\n",
        "            reader = PdfReader(file)\n",
        "            num_pages = len(reader.pages)\n",
        "            print(f\"processing {num_pages} pages...\")\n",
        "\n",
        "            # extract all text.\n",
        "            full_text = []\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "\n",
        "                if text:\n",
        "                    full_text.append(text)\n",
        "                else:\n",
        "                    print(f\"warning: page {page_num + 1} had no extractable text.\")\n",
        "\n",
        "            # combine all pages.\n",
        "            combined_text = \"\\n\\n\".join(full_text)\n",
        "\n",
        "            # save to file if requested.\n",
        "            if save_to_file:\n",
        "                if output_txt_path is None:\n",
        "                    output_txt_path = pdf_path.replace('.pdf', '.txt')\n",
        "\n",
        "                with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "                    txt_file.write(combined_text)\n",
        "\n",
        "                print(f\"saved to: {output_txt_path}\")\n",
        "\n",
        "            return combined_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error reading pdf: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YHwQb_Og96E5"
      },
      "outputs": [],
      "source": [
        "def batch_pdf_to_text(folder_path, output_folder=None):\n",
        "    \"\"\"converts all pdfs in a folder to text files.\"\"\"\n",
        "\n",
        "    # check if folder exists.\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"error: folder '{folder_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    # create output folder if needed.\n",
        "    if output_folder and not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # get all pdf files.\n",
        "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"no pdf files found in folder.\")\n",
        "        return\n",
        "\n",
        "    print(f\"found {len(pdf_files)} pdf files. converting...\")\n",
        "\n",
        "    combined_texts = []\n",
        "\n",
        "    # process each pdf.\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(folder_path, pdf_file)\n",
        "\n",
        "        if output_folder:\n",
        "            output_path = os.path.join(output_folder, pdf_file.replace('.pdf', '.txt'))\n",
        "        else:\n",
        "            output_path = None\n",
        "\n",
        "        print(f\"\\nprocessing: {pdf_file}\")\n",
        "\n",
        "        combined_texts.append(pdf_to_text(pdf_path, output_path))\n",
        "\n",
        "    print(\"\\nbatch conversion complete!\")\n",
        "\n",
        "    return combined_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8CBfx4u-C0K",
        "outputId": "8b489e1b-e386-4a67-8e5a-08f32cb8e4ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found 1 pdf files. converting...\n",
            "\n",
            "processing: Outliers_Malcolm_Gladwell.pdf\n",
            "processing 249 pages...\n",
            "warning: page 1 had no extractable text.\n",
            "saved to: /content/drive/MyDrive/Datasets/Book/Outliers_Malcolm_Gladwell.txt\n",
            "\n",
            "batch conversion complete!\n"
          ]
        }
      ],
      "source": [
        "# batch convert pdfs to text.\n",
        "raw_text_contents = batch_pdf_to_text(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKcxuA23-FWR",
        "outputId": "fd93531c-8970-4ff2-e1c8-814126c5c245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 460987 \n",
            "\n",
            "OUTLIERS\n",
            "The Story of Success\n",
            "MALCOLM GLADWELL\n",
            "BACK BAY BOOKS\n",
            "LITTLE, BROWN AND COMPANY\n",
            "NEW YORK   \n"
          ]
        }
      ],
      "source": [
        "# get first pdf's text content.\n",
        "raw_text_content = raw_text_contents[0]\n",
        "\n",
        "# display character count and preview.\n",
        "print(\"Total number of character:\", len(raw_text_content), \"\\n\")\n",
        "print(raw_text_content[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNS4CqXw-U_m"
      },
      "source": [
        "### **Text Preprocessing**\n",
        "\n",
        "We preprocess the raw text by splitting it on punctuation and whitespace using regex patterns. This function tokenizes the text and removes empty strings, giving us a clean list of words and punctuation marks for building our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iSChh9Pa-UNp"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(raw_text, max_preview=30):\n",
        "    \"\"\"splits text on punctuation and whitespace, removes empty strings.\"\"\"\n",
        "\n",
        "    # split on common punctuation and whitespace.\n",
        "    pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    tokens = re.split(pattern, raw_text)\n",
        "\n",
        "    # remove whitespace and filter empty strings.\n",
        "    preprocessed = [item.strip() for item in tokens if item.strip()]\n",
        "\n",
        "    # preview first n tokens.\n",
        "    if max_preview:\n",
        "        print(f\"first {max_preview} tokens:\")\n",
        "        print(preprocessed[:max_preview])\n",
        "        print(f\"\\ntotal word level tokens: {len(preprocessed)}\\n\")\n",
        "\n",
        "    return preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWZsW8Gf-X7i",
        "outputId": "61c4c932-bf40-4ec3-9934-549f5d692d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first 30 tokens:\n",
            "['OUTLIERS', 'The', 'Story', 'of', 'Success', 'MALCOLM', 'GLADWELL', 'BACK', 'BAY', 'BOOKS', 'LITTLE', ',', 'BROWN', 'AND', 'COMPANY', 'NEW', 'YORK', '•', 'BOSTON', '•', 'LONDON', 'Begin', 'Reading', 'Table', 'of', 'Contents', 'Reading', 'Group', 'Guide', 'Copyright']\n",
            "\n",
            "total word level tokens: 92758\n",
            "\n",
            "11324\n"
          ]
        }
      ],
      "source": [
        "# preprocess and tokenize raw text.\n",
        "preprocessed = preprocess_text(raw_text_content, max_preview=30)\n",
        "\n",
        "# get unique words and vocabulary size.\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mNCA5ui1-aQv"
      },
      "outputs": [],
      "source": [
        "# get all unique tokens and sort them.\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "\n",
        "# add special tokens to vocabulary.\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "# create vocabulary mapping from tokens to integers.\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqV6_7Fc-cEd"
      },
      "source": [
        "<a id=\"word-level-tokenization\"></a>\n",
        "## **Word-Level Tokenization**\n",
        "\n",
        "Word-level tokenization is a straightforward approach where text is split into individual words and punctuation marks. Each unique word in the vocabulary is assigned a specific numerical ID. This method works well for languages with clear word boundaries and limited vocabulary sizes. However, it struggles with rare or unseen words, which are typically replaced with a special `<|unk|>` (unknown) token. This problem is regarded as **out-of-vocabulary words**. While simple to implement and understand, word-level tokenization can result in large vocabularies and poor handling of morphologically rich languages or out-of-vocabulary words.\n",
        "\n",
        "### **Building Custom Word Tokenizer**\n",
        "\n",
        "This Word Tokenizer splits text based on whitespace and punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yNrHerbU-cyE"
      },
      "outputs": [],
      "source": [
        "class WordTokenizer:\n",
        "    \"\"\"tokenizes text into ids and decodes ids back to text. it focuses on word-level tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"initializes tokenizer with vocabulary mappings.\"\"\"\n",
        "\n",
        "        self.tok_to_int = vocab\n",
        "        self.int_to_tok = {integer: token for token, integer in vocab.items()}\n",
        "        self.pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"converts text to list of token ids.\"\"\"\n",
        "\n",
        "        # split on punctuation and whitespace.\n",
        "        preprocessed = re.split(self.pattern, text)\n",
        "\n",
        "        # remove empty strings and whitespace.\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "        # replace unknown tokens with <|unk|>.\n",
        "        preprocessed = [token if token in self.tok_to_int else \"<|unk|>\" for token in preprocessed]\n",
        "\n",
        "        # convert tokens to ids.\n",
        "        ids = [self.tok_to_int[tok] for tok in preprocessed]\n",
        "\n",
        "        # return ids\n",
        "        return ids\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"converts list of token ids back to text.\"\"\"\n",
        "\n",
        "        # map ids to tokens.\n",
        "        tokens = [self.int_to_tok[id] for id in ids]\n",
        "\n",
        "        # join tokens with spaces.\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(self.pattern, r'\\1', text)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(r'\\s+([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-])', r'\\1', text)\n",
        "\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQjJIrkz6Szm",
        "outputId": "d9c9d187-17ed-4430-ce2d-b40f0c5196a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original text: Chris Langan's mother was from San Francisco and was estranged from her family. <|endoftext|> do you know about smartphone cryptocurrency?\n",
            "\n",
            "encoded text: [915, 1905, 11325, 11325, 7500, 10581, 5993, 2676, 1342, 3561, 10581, 5524, 5993, 6370, 5700, 29, 11325, 11325, 11325, 11325, 11325, 5192, 10835, 6940, 3291, 11325, 11325, 430]\n",
            "\n",
            "decoded text: Chris Langan <|unk|> <|unk|> mother was from San Francisco and was estranged from her family. <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> do you know about <|unk|> <|unk|>?\n"
          ]
        }
      ],
      "source": [
        "# initialize the wordtokenizer class.\n",
        "tokenizer = WordTokenizer(vocab)\n",
        "\n",
        "# sample text from (and outside) outliers book.\n",
        "text1 = \"Chris Langan's mother was from San Francisco and was estranged from her family.\"\n",
        "\n",
        "# sample text with unknown words for testing.\n",
        "text2 = \"do you know about smartphone cryptocurrency?\"\n",
        "\n",
        "\n",
        "# combine texts with special separator token.\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(f\"original text: {text}\")\n",
        "\n",
        "# convert text to token ids.\n",
        "encoded_text = tokenizer.encode(text)\n",
        "print(f\"\\nencoded text: {encoded_text}\")\n",
        "\n",
        "# convert token ids back to text.\n",
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(f\"\\ndecoded text: {decoded_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFfPOiXj6Wgx"
      },
      "source": [
        "<a id=\"byte-pair-encoding-bpe\"></a>\n",
        "## **Byte Pair Encoding (BPE) Tokenizer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euIOKC3P7D5e"
      },
      "source": [
        "The GPT Series uses a more sophisticated encoding preprocessing technique to convert human language (texts or words) into sub-word units for its training processes. It uses the Byte Pair Encoding (BPE) tokenizer. BPE tokenizer is fast, performant and compressed compared to other tokenization methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhBjJBaj7DeW"
      },
      "source": [
        "Its implementation is quite complex and time-consuming, so I used the tiktoken open-source library to bypass this process (the logic is almost akin to the WordTokenizer implemented above). Tiktoken is an invertible and lossless tokenization technique. it can work on arbitrary text - even text that is not present in the tokenizer's training data. it compresses the text, whereby making the token sequence shorter than the bytes corresponding to the original text.\n",
        "\n",
        "It also attempts to let the model see common subwords. For instance, \"ing\" is a common subword in English, so BPE encodings will often split \"encoding\" into tokens like \"encod\" and \"ing\" (instead of e.g. \"enc\" and \"oding\"). Because the model will then see the \"ing\" token again and again in different contexts, it helps in better contextual understanding of grammar and in generalization of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOEmQlXQ6YbJ",
        "outputId": "c70c6acc-8c5c-4370-9923-3ae14dcad38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "bpe tokenizer's encoded text: [15645, 16332, 272, 338, 2802, 373, 422, 2986, 6033, 290, 373, 44585, 422, 607, 1641, 13, 220, 50256, 466, 345, 760, 546, 11745, 20210, 30]\n",
            "\n",
            "bpe tokenizer's decoded text: Chris Langan's mother was from San Francisco and was estranged from her family. <|endoftext|> do you know about smartphone cryptocurrency?\n",
            "\n",
            "the vocabulary size of gpt-2 tokenizer is 50257\n"
          ]
        }
      ],
      "source": [
        "# instantiate bpe tokenizer from tiktoken.\n",
        "tokenizer_encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# convert text to token ids.\n",
        "encoded_text = tokenizer_encoder.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(f\"\\nbpe tokenizer's encoded text: {encoded_text}\")\n",
        "\n",
        "# convert token ids back to text.\n",
        "decoded_text = tokenizer_encoder.decode(encoded_text)\n",
        "print(f\"\\nbpe tokenizer's decoded text: {decoded_text}\")\n",
        "\n",
        "# display the gpt-2 bpe tovocabulary size.\n",
        "print(f\"\\nthe vocabulary size of gpt-2 tokenizer is {tokenizer_encoder.n_vocab}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XByQgYbu7QCx"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "    \"\"\"tokenizes text using byte pair encoding (bpe). uses tiktoken's gpt-2 tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"gpt2\"):\n",
        "        \"\"\"initializes bpe tokenizer with specified model.\"\"\"\n",
        "\n",
        "        # load tiktoken encoder for the specified model.\n",
        "        self.tokenizer = tiktoken.get_encoding(model_name)\n",
        "\n",
        "        # obtain and store the tokenizer's vocab size.\n",
        "        self.vocab_size = self.tokenizer.n_vocab\n",
        "\n",
        "    def encode(self, text, allowed_special=None):\n",
        "        \"\"\"converts text to list of token ids.\"\"\"\n",
        "\n",
        "        # set default allowed special tokens.\n",
        "        if allowed_special is None:\n",
        "            allowed_special = {\"<|endoftext|>\"}\n",
        "\n",
        "        # convert text to token ids.\n",
        "        ids = self.tokenizer.encode(text, allowed_special=allowed_special)\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"converts list of token ids back to text.\"\"\"\n",
        "\n",
        "        # convert token ids back to text.\n",
        "        text = self.tokenizer.decode(ids)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"returns the vocabulary size of the tokenizer.\"\"\"\n",
        "\n",
        "        return self.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zWmmoqm7aD-",
        "outputId": "40e80eb3-7b04-4ad4-ef4c-8127c7d85c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total characters: 460987\n",
            "The total tokens: 115003\n"
          ]
        }
      ],
      "source": [
        "# initialize tokenizer.\n",
        "tokenizer = BPETokenizer()\n",
        "\n",
        "# get total character count.\n",
        "total_characters = len(raw_text_content)\n",
        "\n",
        "# get total token count after encoding.\n",
        "total_tokens = len(tokenizer.encode(raw_text_content))\n",
        "\n",
        "print(\"The total characters:\", total_characters)\n",
        "print(\"The total tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UDqEL8LFhc0"
      },
      "source": [
        "## **Next Steps**\n",
        "\n",
        "In the next notebook (`model_architecture.ipynb`), we'll build the gpt series model architecture that uses these tokens as input."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
