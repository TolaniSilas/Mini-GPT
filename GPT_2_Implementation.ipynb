{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mini GPT Implementation**\n",
        "\n",
        "This notebook implements and experiments the Generative Pretrained Transformer (GPT) series (e.g., GPT-2, GPT-3). The model architecture is the foundational building block underlying the GPT family, and its design is inspired by a major breakthrough in deep learning, natural language processing and machine translation: the **self-attention mechanism** introduced in **“Attention Is All You Need”** by Vaswani et al.\n",
        "\n",
        "The models were trained on Moby-Dick book only for experimental purposes."
      ],
      "metadata": {
        "id": "r9J2Jx2oEmIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade pypdf tiktoken"
      ],
      "metadata": {
        "id": "kt8rv0AsQosx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pypdf import PdfReader\n",
        "import re"
      ],
      "metadata": {
        "id": "gF8QaNcHGCKy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load the book data that will be used to train the model**"
      ],
      "metadata": {
        "id": "TQYJ_IxbFO5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I--KM-ONQnNG",
        "outputId": "5e032a6f-3e63-4cd3-ce64-3e8fc7121274"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/MyDrive/Datasets/Book/Outliers_Malcolm_Gladwell.pdf\""
      ],
      "metadata": {
        "id": "_R7S1m0bN7XL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T6ij94vxdDlr"
      },
      "outputs": [],
      "source": [
        "def pdf_to_text(pdf_path, output_txt_path=None, save_to_file=True):\n",
        "    \"\"\"reads pdf and converts to text, optionally saving to .txt file.\"\"\"\n",
        "\n",
        "    # check if pdf exists.\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"error: '{pdf_path}' not found.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # open pdf in binary mode.\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "\n",
        "            # create pdf reader.\n",
        "            reader = PdfReader(file)\n",
        "            num_pages = len(reader.pages)\n",
        "            print(f\"processing {num_pages} pages...\")\n",
        "\n",
        "            # extract all text.\n",
        "            full_text = []\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "\n",
        "                if text:\n",
        "                    full_text.append(text)\n",
        "                else:\n",
        "                    print(f\"warning: page {page_num + 1} had no extractable text.\")\n",
        "\n",
        "            # combine all pages.\n",
        "            combined_text = \"\\n\\n\".join(full_text)\n",
        "\n",
        "            # save to file if requested.\n",
        "            if save_to_file:\n",
        "                if output_txt_path is None:\n",
        "                    output_txt_path = pdf_path.replace('.pdf', '.txt')\n",
        "\n",
        "                with open(output_txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "                    txt_file.write(combined_text)\n",
        "\n",
        "                print(f\"saved to: {output_txt_path}\")\n",
        "\n",
        "            return combined_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error reading pdf: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_pdf_to_text(folder_path, output_folder=None):\n",
        "    \"\"\"converts all pdfs in a folder to text files.\"\"\"\n",
        "\n",
        "    # check if folder exists.\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"error: folder '{folder_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    # create output folder if needed.\n",
        "    if output_folder and not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # get all pdf files.\n",
        "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"no pdf files found in folder.\")\n",
        "        return\n",
        "\n",
        "    print(f\"found {len(pdf_files)} pdf files. converting...\")\n",
        "\n",
        "    # process each pdf.\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(folder_path, pdf_file)\n",
        "\n",
        "        if output_folder:\n",
        "            output_path = os.path.join(output_folder, pdf_file.replace('.pdf', '.txt'))\n",
        "        else:\n",
        "            output_path = None\n",
        "\n",
        "        print(f\"\\nprocessing: {pdf_file}\")\n",
        "        pdf_to_text(pdf_path, output_path)\n",
        "\n",
        "    print(\"\\nbatch conversion complete!\")"
      ],
      "metadata": {
        "id": "U4BJIuhaMw0b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text_content = pdf_to_text(pdf_path)"
      ],
      "metadata": {
        "id": "p5uQYUstMoru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d95ee1-fbd1-491b-a3dd-ef847a8a93d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing 249 pages...\n",
            "warning: page 1 had no extractable text.\n",
            "saved to: /content/drive/MyDrive/Datasets/Book/Outliers_Malcolm_Gladwell.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of character:\", len(raw_text_content))\n",
        "print(raw_text_content[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3dI5aAvZtdU",
        "outputId": "ce70d68b-a955-4293-ef03-caf451fe4fe7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 460995\n",
            "OUTLIERS\n",
            "The Story of Success\n",
            "MALCOLM GLADWELL\n",
            "BACK BAY BOOKS\n",
            "LITTLE, BROWN AND COMPANY\n",
            "NEW YORK   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UkozSO2Ja0JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(raw_text, max_preview=30):\n",
        "    \"\"\"splits text on punctuation and whitespace, removes empty strings.\"\"\"\n",
        "\n",
        "    # split on common punctuation and whitespace.\n",
        "    pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    tokens = re.split(pattern, raw_text)\n",
        "\n",
        "    # remove whitespace and filter empty strings.\n",
        "    preprocessed = [item.strip() for item in tokens if item.strip()]\n",
        "\n",
        "    # preview first n tokens.\n",
        "    if max_preview:\n",
        "        print(f\"first {max_preview} tokens:\")\n",
        "        print(preprocessed[:max_preview])\n",
        "        print(f\"\\ntotal word level tokens: {len(preprocessed)}\\n\")\n",
        "\n",
        "    return preprocessed"
      ],
      "metadata": {
        "id": "Z_N_c68wfJcC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = preprocess_text(raw_text_content, max_preview=30)\n",
        "\n",
        "# get the vocabulary size of the dataset.\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY6gLVk2kyWs",
        "outputId": "95ea0d6f-22cd-4c00-de83-c38ad0cfe638"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 30 tokens:\n",
            "['OUTLIERS', 'The', 'Story', 'of', 'Success', 'MALCOLM', 'GLADWELL', 'BACK', 'BAY', 'BOOKS', 'LITTLE', ',', 'BROWN', 'AND', 'COMPANY', 'NEW', 'YORK', '•', 'BOSTON', '•', 'LONDON', 'Begin', 'Reading', 'Table', 'of', 'Contents', 'Reading', 'Group', 'Guide', 'Copyright']\n",
            "\n",
            "total word level tokens: 92765\n",
            "\n",
            "11327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "raXXtMvDma8g"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z49XMHL-vgai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordTokenizer:\n",
        "    \"\"\"tokenizes text into ids and decodes ids back to text. it focuses on word-level tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"initializes tokenizer with vocabulary mappings.\"\"\"\n",
        "\n",
        "        self.tok_to_int = vocab\n",
        "        self.int_to_tok = {integer: token for token, integer in vocab.items()}\n",
        "        self.pattern = r'([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-]+|\\.\\.\\.|\\s+)'\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"converts text to list of token ids.\"\"\"\n",
        "\n",
        "        # split on punctuation and whitespace.\n",
        "        preprocessed = re.split(self.pattern, text)\n",
        "\n",
        "        # remove empty strings and whitespace.\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "        # replace unknown tokens with <|unk|>.\n",
        "        preprocessed = [token if token in self.tok_to_int else \"<|unk|>\" for token in preprocessed]\n",
        "\n",
        "        # convert tokens to ids.\n",
        "        ids = [self.tok_to_int[tok] for tok in preprocessed]\n",
        "\n",
        "        # return ids\n",
        "        return ids\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"converts list of token ids back to text.\"\"\"\n",
        "\n",
        "        # map ids to tokens.\n",
        "        tokens = [self.int_to_tok[id] for id in ids]\n",
        "\n",
        "        # join tokens with spaces.\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(self.pattern, r'\\1', text)\n",
        "\n",
        "        # remove spaces before punctuation.\n",
        "        text = re.sub(r'\\s+([,.:;?_!\"()\\'\\[\\]{}\\/\\\\|—–-])', r'\\1', text)\n",
        "\n",
        "        return text"
      ],
      "metadata": {
        "id": "jZQAQ4xennON"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the wordtokenizer class.\n",
        "tokenizer = WordTokenizer(vocab)\n",
        "\n",
        "# sample text from (and outside) outliers book.\n",
        "text1 = \"Chris Langan's mother was from San Francisco and was estranged from her family.\"\n",
        "\n",
        "# sample text with unknown words for testing.\n",
        "text2 = \"do you know about smartphone cryptocurrency?\"\n",
        "\n",
        "\n",
        "# combine texts with special separator token.\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(f\"original text: {text}\")\n",
        "\n",
        "# convert text to token ids.\n",
        "encoded_text = tokenizer.encode(text)\n",
        "print(f\"\\nencoded text: {encoded_text}\")\n",
        "\n",
        "# convert token ids back to text.\n",
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(f\"\\ndecoded text: {decoded_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkcOsA0G9tGl",
        "outputId": "113c06e8-92a5-415b-8a1d-b7f30cca3fce"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: Chris Langan's mother was from San Francisco and was estranged from her family. <|endoftext|> do you know about smartphone cryptocurrency?\n",
            "\n",
            "encoded text: [915, 1905, 11328, 8970, 7500, 10584, 5993, 2676, 1342, 3561, 10584, 5524, 5993, 6370, 5700, 29, 11328, 11328, 11328, 11328, 11328, 5192, 10838, 6940, 3291, 11328, 11328, 430]\n",
            "\n",
            "decoded text: Chris Langan <|unk|> s mother was from San Francisco and was estranged from her family. <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> do you know about <|unk|> <|unk|>?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Byte Pair Encoding (BPE) Tokenizer.**"
      ],
      "metadata": {
        "id": "HFdoj2Q1-PXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPT Series uses a more sophisticated encoding preprocessing technique to convert human language (texts or words) into sub-word units for its training processes. It uses the Byte Pair Encoding (BPE) tokenizer. BPE tokenizer is fast, performant and compressed compared to other tokenization methods."
      ],
      "metadata": {
        "id": "5T_pVWgO-Ae3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its implementation is quite complex and time-consuming, so I used the tiktoken open-source library to implement this. tiktoken is an invertible and lossless tokenization technique. it can work on arbitrary text - even text that is not present in the tokenizer's training data. it compresses the text, whereby making the token sequence shorter than the bytes corresponding to the original text.\n",
        "\n",
        "It also attempts to let the model see common subwords. For instance, \"ing\" is a common subword in English, so BPE encodings will often split \"encoding\" into tokens like \"encod\" and \"ing\" (instead of e.g. \"enc\" and \"oding\"). Because the model will then see the \"ing\" token again and again in different contexts, it helps in better contextual understanding of grammar and in generalization of models."
      ],
      "metadata": {
        "id": "0mFnqUFJ-EoW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFBV5PtI-bb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jc1GBmL-bYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5c97zmGj-bQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-cqE-55-bM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}